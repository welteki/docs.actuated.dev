{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"actuated","text":"<p>Actuated brings blazingly fast, secure builds to self-hosted CI runners.</p>"},{"location":"#building-containers-on-self-hosted-runners-is-slow-and-insecure","title":"Building containers on self-hosted runners is slow and insecure","text":"<p>Most solutions that use containers for running Docker or Kubernetes in CI have very poor security boundaries. They require either privileged containers (root on the host), a shared Docker socket (root on the host), third-party tools which don't integrate well and still require root to mount folders, or user namespaces which come with their own limitations. The chances are, if you use Docker or K8s in your CI, and run with: actions-runner-controller, Jenkins, or GitLab, then you may be compromising on security or user experience.</p>"},{"location":"#management-is-a-nightmare","title":"Management is a nightmare","text":"<p>Self-hosted CI runners are continually out of date, and require fine-tuning to get all the right packages in place and Kernel modules to build containers and cloud-native software. You'll also have to spend extra time making sure builds don't conflict, and that they can't cause side effects to system-level packages. What if you need two different version of some software?</p> <p>If you haven't felt this pain yet, then perhaps you're blissfully unaware or are not updating your packages?</p> <p>Are you running privileged containers for CI in your organisation? Are you sharing a Docker Socket (just as bad!)? Are you running Docker in Docker (DIND)? \ud83d\ude48</p>"},{"location":"#self-managed-runners-are-inefficient-and-overprovisioned","title":"Self-managed runners are inefficient and overprovisioned","text":"<p>Self-hosted runners are typically over-provisioned meaning you're spending too much money.</p> <p>Why are they over-provisioned? Because you never know how many jobs you'll have to run, so you have to make them bigger, or have too many hosts available.</p> <p>Why are they inefficient?</p> <p>By default, the self-hosted runner will only schedule one job per host at a time, because GitHub has no knowledge of the capacity of your machines. So each and every build you run could consume all the resources on the host. The second reason is that builds often conflict with one another causing side effects that only happen in CI and are really difficult to track down and reproduce.</p> <p>Actuated uses VMs to slice up the whole machine, and can run many builds in parallel. The net effect is that your build queue will get cleared down much more quickly.</p>"},{"location":"#hands-free-vm-level-isolation","title":"Hands-free, VM-level isolation","text":"<p>Actuated provides a fast-booting microVM which can run Docker, Kubernetes and anything else you need, with full root on the VM, and no access to the host. Each environment is created just in time to take a build, and is removed immediately after.</p> <p>Boot time is usually ~1-2 seconds for the VM, that extra second is because we start Docker as part of the boot-up process.</p> <p>What does \"actuated\" mean?</p> <p>Something that activates or impels itself; specifically (a machine, device, etc.) that causes itself to begin operating automatically, self-activating.</p> <p>We maintain a VM image that is updated regularly through an automated build, so you don't have to install SDKs, runtimes or language packs on your build machines.</p> <p>Just enable automated updates on your server then install the actuated agent. We'll do the rest including managing efficient allocation across your fleet of servers, and updating the CI image.</p> <p>And actuated will run your jobs efficiently across a fleet of hosts, or a single machine. They each need to be either bare-metal hosts (think: AWS Metal / Graviton, Equinix Metal, etc), or support nested virtualization (a feature available on GCP and DigitalOcean)</p>"},{"location":"#what-people-are-saying","title":"What people are saying","text":"<ul> <li> <p>\"We've been piloting Actuated recently. It only took 30s create 5x isolated VMs, run the jobs and tear them down again inside our on-prem environment (no Docker socket mounting shenanigans)! Pretty impressive stuff.\"</p> <p>Addison van den Hoeven - DevOps Lead, Riskfuel</p> </li> <li> <p>\"This is great, perfect for jobs that take forever on normal GitHub runners. I love what Alex is doing here.\"</p> <p>Richard Case, Principal Engineer, SUSE</p> </li> <li> <p>\"Thank you. I think actuated is amazing.\"</p> <p>Alan Sill, NSF Cloud and Autonomic Computing (CAC) Industry-University Cooperative Research Center</p> </li> <li> <p>\"Nice work, security aspects alone with shared/stale envs on self-hosted runners.\"</p> <p>Matt Johnson, Palo Alto Networks</p> </li> <li> <p>\"Is there a way to pay github for runners that suck less?\"</p> <p>Darren Shepherd, Acorn Labs</p> </li> <li> <p>\"Excited to try out actuated! We use custom actions runners and I think there's something here \ud83d\udd25\"</p> <p>Nick Gerace, System Initiative</p> </li> <li> <p>It is awesome to see the work of Alex Ellis with Firecracker VMs. They are provisioning and running GitHub Actions in isolated VMs in seconds (vs minutes).\"</p> <p>Rinat Abdullin, ML &amp; Innovation at Trustbit</p> </li> <li> <p>\"This is awesome!\" (After reducing Parca build time from 33.5 minutes to 1 minute 26s)</p> <p>Frederic Branczyk, Co-founder, Polar Signals</p> </li> </ul>"},{"location":"#watch-a-live-demo","title":"Watch a live demo","text":"<p>Alex shows you how actuated uses an isolated, immutable microVM to run K3s inside of a GitHub Action, followed by a matrix build that causes 5 VMs to be launched. You'll see how quick and easy it is to enable actuated, and how it can buffer and queue up jobs, when there's no remaining capacity in your fleet of agents.</p> <p>You can also watch a webinar that Alex recorded with Richard Case from Weaveworks on how microVMs compare to containers and legacy VMs, you'll see Alex's demo at: 1:13:19.</p>"},{"location":"#conceptual-overview","title":"Conceptual overview","text":"<p>Actuated will schedule builds across your fleet of agents, packing them in densely, without overloading the host. Each microVM will run just one build before being destroyed to ensure a clean, isolated build. </p> <p>Learn more in the FAQ</p>"},{"location":"#get-started","title":"Get started","text":"<ul> <li>Start a subscription or book a call to find out more</li> <li>Read the FAQ</li> <li>Enable actuated for an existing repository</li> <li>Read more in the announcement: Blazing fast CI with MicroVMs</li> </ul>"},{"location":"#comparison","title":"Comparison","text":"<p>Feel free to book a call with us if you'd like to understand this comparison in more detail.</p> Solution Isolated VM Speed Efficient spread of jobs Safely build public repos? ARM64 support Maintenance required Cost Hosted runners Poor None Free minutes in plan <code>1</code> Per build minute actuated Bare-metal Yes Very little Fixed monthly cost Standard self-hosted runners Good DIY Manual setup and updates OSS plus management costs actions-runtime-controller Varies <code>2</code> DIY Very involved OSS plus management costs <p><code>1</code> actions-runtime-controller requires use of separate build tools such as Kaniko, which break the developer experience of using <code>docker</code> or <code>docker-compose</code>. If Docker in Docker (DinD) is used, then there is a severe performance penalty and security risk.</p> <p><code>2</code> Builds on public GitHub repositories are free with the standard hosted runners, however private repositories require billing information, after the initial included minutes are consumed.</p> <p>You can only get VM-level isolation from either GitHub hosted runners or Actuated. Standard self-hosted runners have no isolation between builds and actions-runtime-controller requires either a Docker socket to be mounted or Docker In Docker (a privileged container) to build and run containers.</p>"},{"location":"#got-questions-comments-or-suggestions","title":"Got questions, comments or suggestions?","text":"<p>actuated is trademark of OpenFaaS Ltd.</p> <p>You can contact the team working on actuated via email at: contact@openfaas.com</p> <p>Follow @selfactuated on Twitter for updates and announcements</p>"},{"location":"contact/","title":"Contact us","text":""},{"location":"contact/#contact-us","title":"Contact us","text":"<p>Would you like to contact us about actuated for your team or oranisation?</p> <p>Fill out this form, and we'll get in touch shortly after with next steps.</p> <p>Actuated \u2122 is a trademark of OpenFaaS Ltd.</p>"},{"location":"contact/#keeping-in-touch","title":"Keeping in touch","text":"<ul> <li>Follow us on Twitter - @selfactuated</li> <li>GitHub - github.com/self-actuated</li> <li>Customer Slack - self-actuated.slack.com</li> </ul>"},{"location":"contact/#anything-else","title":"Anything else?","text":"<p>Looking for technical details about actuated? Try the FAQ.</p> <p>Are you running into a problem? Try the troubleshooting guide</p>"},{"location":"dashboard/","title":"Actuated Dashboard","text":"<p>The actuated dashboard is available to customers for their enrolled organisations.</p> <p>For each organisation, you can see:</p> <ul> <li>Today's builds so far - a quick picture of today's activity across all enrolled organisations</li> <li>Runners - Your build servers and their status</li> <li>Build queue - All builds queued for processing and their status</li> <li>Insights - full build history and usage by organisation, repo and user</li> </ul> <p>Plus:</p> <ul> <li>CLI - install the CLI for management via command line</li> <li>SSH Sessions - connect to a runner via SSH to debug issues or to explore - works on hosted and actuated runners</li> </ul>"},{"location":"dashboard/#todays-builds-so-far","title":"Today's builds so far","text":"<p>On this page, you'll get today's total builds, total build minutes and a break-down on the statuses - to see if you have a majority of successful or unsuccessful builds.</p> <p></p> <p>Today's activity at a glance</p> <p>Underneath this section, there are a number of tips for enabling a container cache, adjusting your subscription plan and for joining the actuated Slack.</p> <p>More detailed reports are available on the insights page.</p>"},{"location":"dashboard/#runners","title":"Runners","text":"<p>Here you'll see if any of your servers are offline, in a draining status due a restart/update or online and ready to process builds.</p> <p></p> <p>The Ping time is how long it takes for the control-plane to check the agent's capacity.</p>"},{"location":"dashboard/#build-queue","title":"Build queue","text":"<p>Only builds that are queued (not yet in progress), or already in progress will be shown on this page.</p> <p></p> <p>Find out how many builds are pending or running across your organisation and on which servers.</p>"},{"location":"dashboard/#insights","title":"Insights","text":"<p>Three sets of insights are offered - all at the organisation level, so every repository is taken into account.</p> <p>You can also switch the time window between 28 days, 14 days, 7 days or today.</p> <p>The data is contrasted to the previous period to help you identify spikes and potential issues.</p> <p>The data for reports always starts from the last complete day of data, so the last 7 days will start from the previous day.</p>"},{"location":"dashboard/#build-history-and-usage-by-organisation","title":"Build history and usage by organisation.","text":"<p>Understand when your builds are running at a higher level - across all of your organisations - in one place.</p> <p></p> <p>You can click on Minutes to switch to total time instead of total builds, to see if the demand on your servers is increasing or decreasing over time.</p>"},{"location":"dashboard/#build-history-by-repository","title":"Build history by repository","text":"<p>When viewing usage at a repository-level, you can easily identify anomalies and hot spots - like mounting build times, high failure rates or lots of cancelled jobs - implying a potential faulty interaction or trigger.</p>"},{"location":"dashboard/#build-history-per-user","title":"Build history per user","text":"<p>This is where you get to learn who is trigger the most amount of builds, who may be a little less active for this period and where folks may benefit from additional training due a high failure rate of builds.</p>"},{"location":"dashboard/#ssh-sessions","title":"SSH Sessions","text":"<p>Once you configure an action to pause at a set point by introducing our custom GitHub action step, you'll be able to copy and paste an SSH command and run it in your terminal.</p> <p>Your SSH keys will be pre-installed and no password is required.</p> <p></p> <p>Viewing an SSH session to a hosted runner</p> <p>See also: Example: Debug a job with SSH</p>"},{"location":"dashboard/#cli","title":"CLI","text":"<p>The CLI page has download instructions, you can find the downloads for Linux, macOS and Windows here:</p> <p>self-actuated/actuated-cli</p>"},{"location":"expose-agent/","title":"Expose agent","text":""},{"location":"expose-agent/#expose-the-agents-api-over-https","title":"Expose the agent's API over HTTPS","text":"<p>The actuated agent serves HTTP, and must be accessible by the actuated control plane.</p> <p>We expect most of our customers to be using hosts with public IP addresses, and the combination of an API token plus TLS is a battle tested combination.</p> <p>For anyone running with private hosts, OpenFaaS Ltd's inlets product can be used to get incoming traffic over a secure tunnel</p>"},{"location":"expose-agent/#for-a-host-on-a-public-cloud","title":"For a host on a public cloud","text":"<p>If you're running the agent on a host with a public IP, you can use the built-in TLS mechanism in the actuated agent to receive a certificate from Let's Encrypt, valid for 90 days. The certificate will be renewed by the actuated agent, so there are no additional administration tasks required.</p> <p></p> <p>Pictured: Accessing the agent's endpoint built-in TLS and Let's Encrypt</p> <p>Determine the public IP of your instance:</p> <pre><code># curl -s http://checkip.amazonaws.com\n\n141.73.80.100\n</code></pre> <p>Now imagine that your sub-domain is <code>agent.example.com</code>, you need to create a DNS A record of <code>agent.example.com=141.73.80.100</code>, changing both the sub-domain and IP to your own.</p> <p>Once the agent is installed, edit /etc/default/actuated on the agent and set the following two variables:</p> <pre><code>AGENT_LETSENCRYPT_DOMAIN=\"agent.example.com\"\nAGENT_LETSENCRYPT_EMAIL=\"webmaster@agent.example.com\"\n</code></pre> <p>Restart the agent:</p> <pre><code>sudo systemctl restart actuated\n</code></pre> <p>Your agent's endpoint URL is going to be: <code>https://agent.example.com</code> on port 443</p>"},{"location":"expose-agent/#private-hosts-on-premises-behind-nat-or-at-home","title":"Private hosts - on-premises, behind NAT or at home","text":"<p>You'll need a way to expose the client to the Internet, which includes HTTPS encryption and a sufficient amount of connections/traffic per minute.</p> <p>Inlets provides a quick and secure solution here. It is available on a monthly subscription, bear in mind that the \"Personal\" plan is not for this kind of commercial use.</p> <p></p> <p>Pictured: Accessing the agent's private endpoint using an inlets-pro tunnel</p> <p>Reach out to us if you'd like us to host a tunnel server for you, alternatively, you can follow the instructions below to set up your own.</p> <p>The inletsctl tool will create a HTTPS tunnel server with you on your favourite cloud with a HTTPS certificate obtained from Let's Encrypt.</p> <p>If you have just the one Actuated Agent:</p> <pre><code>export AGENT_DOMAIN=agent1.example.com\nexport LE_EMAIL=webmaster@agent1.example.com\n\narkade get inletsctl\nsudo mv $HOME/.arkade/bin/inletsctl /usr/local/bin/\n\ninletsctl create \\\n--provider digitalocean \\\n--region lon1 \\\n--token-file $HOME/do-token \\\n--letsencrypt-email $LE_EMAIL \\\n--letsencrypt-domain $AGENT_DOMAIN\n</code></pre> <p>Then note down the tunnel's wss:// URL and token.</p> <p>Then run a HTTPS client to expose your agent:</p> <pre><code>inlets-pro http client \\\n--url $WSS_URL \\\n--token $TOKEN \\\n--upstream http://127.0.0.1:8081\n</code></pre> <p>For two or more Actuated Servers:</p> <pre><code>export AGENT_DOMAIN1=agent1.example.com\nexport AGENT_DOMAIN2=agent2.example.com\nexport LE_EMAIL=webmaster@agent1.example.com\n\narkade get inletsctl\nsudo mv $HOME/.arkade/bin/inletsctl /usr/local/bin/\n\ninletsctl create \\\n--provider digitalocean \\\n--region lon1 \\\n--token-file $HOME/do-token \\\n--letsencrypt-email $LE_EMAIL \\\n--letsencrypt-domain $AGENT_DOMAIN1 \\\n--letsencrypt-domain $AGENT_DOMAIN2\n</code></pre> <p>Then note down the tunnel's wss:// URL and token.</p> <p>Then run a HTTPS client to expose your agent, using the unique agent domain, run the inlets-pro client on the Actuated Servers:</p> <pre><code>export AGENT_DOMAIN1=agent1.example.com\ninlets-pro http client \\\n--url $WSS_URL \\\n--token $TOKEN \\\n--upstream $AGENT1_DOMAIN=http://127.0.0.1:8081\n</code></pre> <pre><code>export AGENT_DOMAIN2=agent2.example.com\ninlets-pro http client \\\n--url $WSS_URL \\\n--token $TOKEN \\\n--upstream $AGENT1_DOMAIN=http://127.0.0.1:8081\n</code></pre> <p>You can generate a systemd service (so that inlets restarts upon disconnection, and reboot) by adding <code>--generate=systemd &gt; inlets.service</code> and running:</p> <pre><code>sudo cp inlets.service /etc/systemd/system/\nsudo systemctl daemon-reload\nsudo systemctl enable inlets.service\nsudo systemctl start inlets\n\n# Check status with:\nsudo systemctl status inlets\n</code></pre> <p>Your agent's endpoint URL is going to be: <code>https://$AGENT_DOMAIN</code>.</p>"},{"location":"expose-agent/#preventing-the-runner-from-accessing-your-local-network","title":"Preventing the runner from accessing your local network","text":"<p>Network segmentation</p> <p>Proper network segmentation of hosts running the actuated agent is required. This is to prevent runners from making outbound connections to other hosts on your local network. We will not accept any responsibility for your configuration.</p> <p>If hardware isolation is not available, iptables rules may provide an alternative for isolating the runners from your network.</p> <p>Imagine you were using a LAN range of <code>192.168.0.0/24</code>, with a router of <code>192.168.0.1</code>, then the following probes and tests show that the runner cannot access the host 192.168.0.101, and that nmap's scan will come up dry.</p> <p>We add a rule to allow access to the router, but reject packets going via TCP or UDP to any other hosts on the network.</p> <pre><code>sudo iptables --insert CNI-ADMIN \\\n--destination  192.168.0.1 --jump ACCEPT\nsudo iptables --insert CNI-ADMIN \\\n--destination  192.168.0.0/24 --jump REJECT -p tcp  --reject-with tcp-reset\nsudo iptables --insert CNI-ADMIN \\\n--destination  192.168.0.0/24 --jump REJECT -p udp --reject-with icmp-port-unreachable\n</code></pre> <p>You can test the efficacy of these rules by running nmap, mtr, ping and any other probing utilities within a GitHub workflow.</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\n\njobs:\nspecs:\nname: specs\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v1\n- name: addr\nrun: ip addr\n- name: route\nrun: ip route\n- name: pkgs\nrun: |\nsudo apt-get update &amp;&amp; \\\nsudo apt-get install traceroute mtr nmap netcat -qy\n- name: traceroute\nrun: traceroute  192.168.0.101\n- name: Connect to ssh\nrun: echo | nc  192.168.0.101 22\n- name: mtr\nrun: mtr -rw  -c 1  192.168.0.101\n- name: nmap for SSH\nrun: nmap -p 22  192.168.0.0/24\n- name: Ping router\nrun: |\nping -c 1  192.168.0.1\n- name: Ping 101\nrun: |\nping -c 1  192.168.0.101\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#how-does-it-work","title":"How does it work?","text":"<p>Actuated has three main parts:</p> <ol> <li>an agent which knows how to run VMs, you install this on your hosts</li> <li>a VM image and Kernel that we build which has everything required for Docker, KinD and K3s</li> <li>a multi-tenant control plane that we host, which tells your agents to start VMs and register a runner on your GitHub organisation</li> </ol> <p>The multi-tenant control plane is run and operated by OpenFaaS Ltd as a SaaS.</p> <p></p> <p>The conceptual overview showing how a MicroVM is requested by the control plane.</p> <p>MicroVMs are only started when needed, and are registered with GitHub by the official GitHub Actions runner, using a short-lived registration token. The token is been encrypted with the public key of the agent. This ensures no other agent could use the token to bootstrap a token to the wrong organisation.</p> <p>Learn more: Self-hosted GitHub Actions API</p>"},{"location":"faq/#glossary","title":"Glossary","text":"<ul> <li><code>MicroVM</code> - a lightweight, single-use VM that is created by the Actuated Agent, and is destroyed after the build is complete. Common examples include firecracker by AWS and Cloud Hypervisor</li> <li><code>Guest Kernel</code> - a Linux kernel that is used together with a Root filesystem to boot a MicroVM and run your CI workloads</li> <li><code>Root filesystem</code> - an immutable image maintained by the actuated team containing all necessary software to perform a build</li> <li><code>Actuated</code> ('Control Plane') - a multi-tenant SaaS run by the actuated team responsible for scheduling MicroVMs to the Actuated Agent </li> <li><code>Actuated Agent</code> - the software component installed on your Server which runs a MicroVM when instructed by Actuated</li> <li><code>Actuated Server</code> ('Server') - a server on which the Actuated Agent has been installed, where your builds will execute.</li> </ul>"},{"location":"faq/#how-does-actuated-compare-to-a-self-hosted-runner","title":"How does actuated compare to a self-hosted runner?","text":"<p>A self-hosted runner is a machine on which you've installed and registered the a GitHub runner.</p> <p>Quite often these machines suffer from some, if not all of the following issues:</p> <ul> <li>They require several hours to get all the required packages correctly installed to mirror a hosted runner</li> <li>You never update them out of fear of wasting time or breaking something which is working, meaning your supply chain is at risk</li> <li>Builds clash, if you're building a container image, or running a KinD cluster, names will clash, dirty state will be left over</li> </ul> <p>We've heard in user interviews that the final point of dirty state can cause engineers to waste several days of effort chasing down problems.</p> <p>Actuated uses a one-shot VM that is destroyed immediately after a build is completed.</p>"},{"location":"faq/#who-is-actuated-for","title":"Who is actuated for?","text":"<p>actuated is primarily for software engineering teams who are currently using GitHub Actions. A GitHub organisation is required for installation, and runners are attached to individual repositories as required, to execute builds.</p>"},{"location":"faq/#is-there-a-sponsored-subscription-for-open-source-projects","title":"Is there a sponsored subscription for Open Source projects?","text":"<p>We have a sponsored program with the CNCF and Ampere for various Open Source projects, you can find out more here: Announcing managed Arm CI for CNCF projects.</p> <p>Sponsored projects are required to add our GitHub badge to the top of their README file for each repository where the actuated is being used, along with any other GitHub badges such as build status, code coverage, etc.</p> <pre><code>&lt;a href=\"https://actuated.dev/\"&gt;&lt;img alt=\"Arm CI sponsored by Actuated\" src=\"https://docs.actuated.dev/images/actuated-badge.png\" width=\"120px\"&gt;&lt;/img&gt;&lt;/a&gt;\n</code></pre> <p>For an example of what this would look like, see the inletsctl project README.</p>"},{"location":"faq/#what-kind-of-machines-do-i-need-for-the-agent","title":"What kind of machines do I need for the agent?","text":"<p>You'll need either: a bare-metal host (your own, AWS i3.metal or Equinix Metal), or a VM that supports nested virtualisation such as those provided by GCP, DigitalOcean and Azure.</p>"},{"location":"faq/#when-will-jenkins-gitlab-ci-bitbucket-pipeline-runners-drone-or-azure-devops-be-supported","title":"When will Jenkins, GitLab CI, BitBucket Pipeline Runners, Drone or Azure DevOps be supported?","text":"<p>For the pilot phase, we're targeting GitHub Actions because it has fine-grained access controls and the ability to schedule exactly one build to a runner. Most other CI systems expect self-hosted runners to perform many builds, and we believe that to be an anti-pattern. We'll offer advice to teams accepted into the pilot who wish to evaluate GitHub Actions or migrate away from another solution.</p> <p>That said, if you're using these tools within your organisation, and face similar issues or concerns, we'd like to hear from you. And we have a proof of concept that works with GitLab CI, so feel free to reach out to us if you feel actuated would be a good fit for your team.</p> <p>Watch the actuated for GitLab preview</p> <p>Feel free to contact us at: contact@openfaas.com</p>"},{"location":"faq/#is-github-enterprise-supported","title":"Is GitHub Enterprise supported?","text":"<p>GitHub.com's Pro, Team and Enterprise Cloud plans are supported.</p> <p>GitHub Enterprise Server (GHES) is a self-hosted version of GitHub and may require additional configuration. Please reach out to us if you're interested in using actuated with your installation of GHES.</p>"},{"location":"faq/#what-kind-of-access-is-required-to-my-github-organisation","title":"What kind of access is required to my GitHub Organisation?","text":"<p>GitHub Apps provide fine-grained privileges, access control, and event data.</p> <p>Actuated integrates with GitHub using a GitHub App.</p> <p>The actuated GitHub App will request:</p> <ul> <li>Administrative access to add/remove GitHub Actions Runners to individual repositories</li> <li>Events via webhook for Workflow Runs and Workflow Jobs</li> </ul> <p>Did you know? The actuated service does not need any access to your code or private or public repositories.</p>"},{"location":"faq/#can-githubs-self-hosted-runner-be-used-on-public-repos","title":"Can GitHub's self-hosted runner be used on public repos?","text":"<p>The GitHub team recommends only running their self-hosted runners on private repositories.</p> <p>Why?</p> <p>I took some time to ask one of the engineers on the GitHub Actions team.</p> <p>With the standard self-hosted runner, a bad actor could compromise the system or install malware leaving side-effects for future builds.</p> <p>He replied that it's difficult for maintainers to secure their repos and workflows, and that bad actors could compromise a runner host due to the way they run multiple jobs, and are not a fresh environment for each build. It may even be because a bad actor could scan the local network of the runner and attempt to gain access to other systems.</p> <p>If you're wondering whether containers and Pods are a suitable isolation level, we would recommend against this since it usually involves one of either: mounting a docker socket (which can lead to escalation to root on the host) or running Docker In Docker (DIND) which requires a privileged container (which can lead to escalation to root on the host).</p> <p>So, can you use actuated on a public repo?</p> <p>Our contact at GitHub stated that through VM-level isolation and an immutable VM image, the primary concerns is resolved, because there is no way to have state left over or side effects from previous builds.</p> <p>Actuated fixes the isolation problem, and prevents side-effects between builds. We also have specific iptables rules in the troubleshooting guide which will isolate your runners from the rest of the network.</p>"},{"location":"faq/#can-i-use-the-containers-feature-of-github-actions","title":"Can I use the containers feature of GitHub Actions?","text":"<p>GitHub Action's Running jobs in a container feature is supported, as is Docker, Buildx, Kubernetes, KinD, K3s, eBPF, etc.</p> <p>Example of running commands with the <code>docker.io/node:16</code> image.</p> <pre><code>jobs:\nspecs:\nname: test\nruns-on: actuated\ncontainer:\nimage: docker.io/node:16\nenv:\nNODE_ENV: development\nports:\n- 3000\noptions: --cpus 1\nsteps:\n- name: Check for dockerenv file\nrun: node --version\n</code></pre>"},{"location":"faq/#how-many-builds-does-a-single-actuated-vm-run","title":"How many builds does a single actuated VM run?","text":"<p>When a VM starts up, it runs the GitHub Actions Runner ephemeral (aka one-shot) mode, so in can run at most one build. After that, the VM will be destroyed.</p> <p>See also: GitHub: ephemeral runners</p>"},{"location":"faq/#how-are-vms-scheduled","title":"How are VMs scheduled?","text":"<p>VMs are placed efficiently across your Actuated Servers using a scheduling algorithm based upon the amount of RAM reserved for the VM.</p> <p>Autoscaling of VMs is automatic. Let's say that you had 10 jobs pending, but given the RAM configuration, only enough capacity to run 8 of them? The second two would be queued until capacity one or more of those 8 jobs completed.</p> <p>If you find yourself regularly getting into a queued state, there are three potential changes to consider:</p> <ol> <li>Using Actuated Servers with more RAM</li> <li>Allocated less RAM to each job</li> <li>Adding more Actuated Servers</li> </ol> <p>The plan you select will determine how many Actuated Servers you can run, so consider 1. and 2. before 3.</p>"},{"location":"faq/#do-i-need-to-auto-scale-the-actuated-servers","title":"Do I need to auto-scale the Actuated Servers?","text":"<p>Please read the section \"How are VMs scheduled\".</p> <p>Auto-scaling Pods or VMs is a quick, painless operation that makes sense for customer traffic, which is generally unpredictable and can be very bursty.</p> <p>GitHub Actions tends to be driven by your internal development team, with a predictable pattern of activity. It's unlikely to vary massively day by day, which means autoscaling is less important than with a user-facing website.</p> <p>In addition to that, bare-metal servers can take 5-10 minutes to provision and may even include a setup fee or monthly commitment, meaning that what you're used to seeing with Kubernetes or AWS Autoscaling Groups may not translate well, or even be required for CI.</p> <p>If you are cost sensitive, you should review the options under Provision a Server section.</p> <p>Depending on your provider, you may also be able to hibernate or suspend servers on a cron schedule to save a few dollars. Actuated will hold jobs in a queue until a server is ready to take them again.</p>"},{"location":"faq/#what-do-i-need-to-change-in-my-workflows-to-use-actuated","title":"What do I need to change in my workflows to use actuated?","text":"<p>Very little, just add / set <code>runs-on: actuated</code></p>"},{"location":"faq/#is-arm64-supported","title":"Is ARM64 supported?","text":"<p>Yes, actuated is built to run on both Intel/AMD and ARM64 hosts, check your subscription plan to see if ARM64 is included. This includes a Raspberry Pi 4B, AWS Graviton, Oracle Cloud Arm instances and potentially any other ARM64 instances which support virtualisation.</p>"},{"location":"faq/#whats-in-the-vm-image-and-how-is-it-built","title":"What's in the VM image and how is it built?","text":"<p>The VM image contains similar software to the hosted runner image: <code>ubuntu-latest</code> offered by GitHub. Unfortunately, GitHub does not publish this image, so we've done our best through user-testing to reconstruct it, including all the Kernel modules required to run Kubernetes and Docker.</p> <p>The image is built automatically using GitHub Actions and is available on a container registry.</p>"},{"location":"faq/#what-kernel-version-is-being-used","title":"What Kernel version is being used?","text":"<p>At time of writing, actuated is using a 5.10.201 Kernel version, and will upgrade to various patch versions as they become available.</p> <p>As and when newer versions are made available, we'll upgrade our support.</p> <p>The Firecracker team has released a guest configuration for the 6.1 Kernel, however there are several known issues which need to be addressed before it can be used with actuated.</p>"},{"location":"faq/#where-are-the-kernel-headers-includes","title":"Where are the Kernel headers / includes?","text":"<p>Warning</p> <p>The following command is only designed for off the shelf cloud image builds of Ubuntu server, and will not work on actuated.</p> <pre><code>apt-get install linux-headers-$(uname -r) </code></pre> <p>For actuated, you'll need to take a different approach to build a DKMS or kmod module for your Kernel.</p> <p>Add self-actuated/get-kernel-sources to your workflow and run it before your build step.</p> <pre><code>      - name: Install kernel headers (actuated)\nuses: self-actuated/get-kernel-sources@master\n</code></pre> <p>An <code>if</code> statement can be added to the block, if you also run the same job on various other types of runners outside of actuated.</p>"},{"location":"faq/#where-is-the-kernel-configuration","title":"Where is the Kernel configuration?","text":"<p>You can run a job to print out or dump the configuration from proc, or from /boot/.</p> <p>Just create a new job, or an SSH debug session and run:</p> <pre><code>sudo modprobe configs\ncat /proc/config.gz | gunzip &gt; /tmp/config\n\n# Look for a specific config option\ncat /tmp/config | grep \"CONFIG_DEBUG_INFO_BTF\"\n</code></pre>"},{"location":"faq/#how-easy-is-it-to-debug-a-runner","title":"How easy is it to debug a runner?","text":"<p>OpenSSH is pre-installed, but it will be inaccessible from your workstation by default.</p> <p>To connect, you can use an inlets tunnel, Wireguard VPN or Tailscale ephemeral token (remember: Tailscale is not free for your commercial use) to log into any agent.</p> <p>We also offer a SSH gateway in some of our tiers, tell us if this is important to you in your initial contact, or reach out to us via email if you're already a customer.</p> <p>See also: Debug a GitHub Action with SSH</p>"},{"location":"faq/#comparison-to-other-solutions","title":"Comparison to other solutions","text":"<p>Feel free to book a call with us if you'd like to understand this comparison in more detail.</p> Solution Isolated VM Speed Efficient spread of jobs Safely build public repos? ARM64 support Maintenance required Cost Hosted runners Poor None Free minutes in plan <code>*</code> Per build minute actuated Bare-metal Yes Very little Fixed monthly cost Standard self-hosted runners Good DIY Manual setup and updates OSS plus management costs actions-runtime-controller Varies <code>*</code> DIY Very involved OSS plus management costs <p><code>1</code> actions-runtime-controller requires use of separate build tools such as Kaniko, which break the developer experience of using <code>docker</code> or <code>docker-compose</code>. If Docker in Docker (DinD) is used, then there is a severe performance penalty and security risk.</p> <p><code>2</code> Builds on public GitHub repositories are free with the standard hosted runners, however private repositories require billing information, after the initial included minutes are consumed.</p> <p>You can only get VM-level isolation from either GitHub hosted runners or Actuated. Standard self-hosted runners have no isolation between builds and actions-runtime-controller requires either a Docker socket to be mounted or Docker In Docker (a privileged container) to build and run containers.</p>"},{"location":"faq/#what-about-iam-permissions-for-aws","title":"What about IAM permissions for AWS?","text":"<p>If you need to publish images to Amazon Elastic Container Registry (ECR), you can either assign a role to any EC2 bare-metal instances that you're using with actuated, or use GitHub's built-in OpenID Connect support.</p> <p>Web Identity Federation means that a job can assume a role within AWS using Secure Token Service (STS) without needing any long-lived credentials.</p> <p>Read more: Configuring OpenID Connect in Amazon Web Services</p>"},{"location":"faq/#how-does-actuated-compare-to-a-actions-runtime-controller-arc","title":"How does actuated compare to a actions-runtime-controller (ARC)?","text":"<p>actions-runtime-controller (ARC) describes itself as \"still in its early stage of development\". It was created by an individual developer called Yusuke Kuoka, and now receives updates from GitHub's team, after having been adopted into the actions GitHub Organisation.</p> <p>Its primary use-case is scale GitHub's self-hosted actions runner using Pods in a Kubernetes cluster. ARC is self-hosted software which means its setup and operation are complex, requiring you to create an properly configure a GitHub App along with its keys. For actuated, you only need to run a single binary on each of your runner hosts and send us an encrypted bootstrap token.</p> <p>If you're running <code>npm install</code> or <code>maven</code>, then this may be a suitable isolation boundary for you.</p> <p>The default mode for ARC is a reuseable runner, which can run many jobs, and each job could leave side-effects or poison the runner for future job runs.</p> <p>If you need to build a container, in a container, on a Kubernetes node offers little isolation or security boundary.</p> <p>What if ARC is configured to use \"rootless\" containers? With a rootless container, you lose access to \"root\" and <code>sudo</code>, both of which are essential in any kind of CI job. Actuated users get full access to root, and can run <code>docker build</code> without any tricks or losing access to <code>sudo</code>. That's the same experience you get from a hosted runner by GitHub, but it's faster because it's on your own hardware.</p> <p>You can even run minikube, KinD, K3s and OpenShift with actuated without any changes.</p> <p>ARC runs a container, so that should work on any machine with a modern Kernel, however actuated runs a VM, in order to provide proper isolation.</p> <p>That means ARC runners can run pretty much anywhere, but actuated runners need to be on a bare-metal machine, or a VM that supports nested virtualisation.</p> <p>See also: Where can I run my agents?</p>"},{"location":"faq/#doesnt-kaniko-fix-all-this-for-arc","title":"Doesn't Kaniko fix all this for ARC?","text":"<p>Kaniko, by Google is an open source project for building containers. It's usually run as a container itself, and usually will require root privileges in order to mount the various filesystems layers required.</p> <p>See also: Root user inside a container is root on the host</p> <p>If you're an ARC user and for various reasons, cannot migrate away to a more secure solution like actuated, Kaniko may be a step in the right direction. Google Cloud users could also create a dedicated node pool with gVisor enabled, for some additional isolation.</p> <p>However, it can only build containers, and still requires root, and itself is often run in Docker, so we're getting back to the same problems that actuated set out to solve.</p> <p>In addition, Kaniko cannot and will not help you to run that container that you've just built to validate it to run end to end tests, neither can it run a KinD cluster, or a Minikube cluster.</p>"},{"location":"faq/#do-we-need-to-run-my-actuated-servers-247","title":"Do we need to run my Actuated Servers 24/7?","text":"<p>Let's say that you wanted to access a single ARM64 runner to speed up your Arm builds from 33 minutes to &lt; 2 minutes like in this example.</p> <p>The two cheapest options for ARM64 hardware would be:</p> <ul> <li>Buy a Mac Mini M1, host it in your office or a co-lo with Asahi Linux installed. That's a one-time cost and will last for several years.</li> <li>Or you could rent an AWS a1.metal by the hour from AWS with very little up front cost, and pay for the time you use it.</li> </ul> <p>In both cases, we're not talking about a significant amount of money, however we are sometimes asked about whether Actuated Servers need to be running 24/7.</p> <p>The answer if that it's a trade-off between cost and convenience. We recommend running them continually, however you can turn them off when you're not using them if you think it is worth your time to do so.</p> <p>If you only needed to run Arm builds from 9-5pm, you could absolutely delete the VM and re-create it with a cron job, just make sure you restore the required files from the original registration of the agent. You may also be able to \"suspend\" or \"hibernate\" the host at a reduced cost, this depends on the hosting provider. Feel free to reach out to us if you need help with this.</p>"},{"location":"faq/#is-there-gpu-support","title":"Is there GPU support?","text":"<p>We are currently exploring dedicating a GPU to a build. So if an Actuated Server had 8x GPUs, you could run 8x GPU-based builds on that host at once, each with one GPU, or 2x jobs with 4x GPUS etc. Let us know if this is something you need when you get in touch with us.</p>"},{"location":"faq/#can-virtual-machines-be-launched-within-a-github-action","title":"Can Virtual Machines be launched within a GitHub Action?","text":"<p>It is possible to launch a Virtual Machine (VM) with KVM from within a Firecracker MicroVM.</p> <p>Use-cases may include: building and snapshotting VM images, running Packer, launching VirtualBox and Vagrant, accelerating the Android emulator, building packages for NixOS and other testing which requires KVM.</p> <p>It's disabled by default, but you can opt-in to the feature by following the steps in this article:</p> <p>How to run a KVM guest in your GitHub Actions</p> <p>At time of writing, only Intel and AMD CPUs support nested virtualisation. This may be on by default, but if not, you can enable it in the system's BIOS or out of band console.</p>"},{"location":"faq/#is-windows-or-macos-supported","title":"Is Windows or MacOS supported?","text":"<p>Linux is the only supported platform for actuated at this time on a AMD64 or ARM64 architecture. We may consider other operating systems in the future, feel free to reach out to us.</p>"},{"location":"faq/#is-actuated-free-and-open-source","title":"Is Actuated free and open-source?","text":"<p>Actuated currently uses the Firecracker project to launch MicroVMs to isolate jobs during CI. Firecracker is an open source Virtual Machine Manager used by Amazon Web Services (AWS) to run serverless-style workloads for AWS Lambda.</p> <p>Actuated is a commercial B2B product and service created and operated by OpenFaaS Ltd.</p> <p>Read the End User License Agreement (EULA)</p> <p>The website and documentation are available on GitHub and we plan to release some open source tools in the future to improve customer experience.</p>"},{"location":"faq/#is-there-a-risk-that-we-could-get-locked-in-to-actuated","title":"Is there a risk that we could get \"locked-in\" to actuated?","text":"<p>No, you can move back to either hosted runners (pay per minute from GitHub) or self-managed self-hosted runners at any time. Bear in mind that actuated solves painful issues with both hosted runners and self-managed self-hosted runners.</p>"},{"location":"faq/#why-is-the-brand-called-actuated-and-selfactuated","title":"Why is the brand called \"actuated\" and \"selfactuated\"?","text":"<p>The name of the software, product and brand is: \"actuated\". In some places \"actuated\" is not available, and we liked \"selfactuated\" more than \"actuatedhq\" or \"actuatedio\" because it refers to the hybrid experience of self-hosted runners.</p>"},{"location":"faq/#privacy-policy-data-security","title":"Privacy policy &amp; data security","text":"<p>Actuated is a managed service operated by OpenFaaS Ltd, registered company number: 11076587.</p> <p>It has both a Software as a Service (SaaS) component (\"control plane\") aka (\"Actuated\") and an agent (\"Actuated Agent\"), which runs on a Server supplied by the customer (\"Customer Server\").</p>"},{"location":"faq/#data-storage","title":"Data storage","text":"<p>The control-plane of actuated collects and stores:</p> <ul> <li>Job events for the organisation where a label of \"actuated*\" is found, including:<ul> <li>Organisation name</li> <li>Repository name</li> <li>Actor name for each job</li> <li>Build name</li> <li>Build start / stop time</li> <li>Build status</li> </ul> </li> </ul> <p>The following is collected from agents:</p> <ul> <li>Agent version</li> <li>Hostname &amp; uptime</li> <li>Platform information - Operating System and architecture</li> <li>System capacity - total and available RAM &amp; CPU</li> </ul> <p>In addition, for support requests, we may need to collect the logs of the actuated agent process remotely from:</p> <ul> <li>VMs launched for jobs, stored at <code>/var/log/actuated/</code></li> </ul> <p>This information is required to operate the control plane including scheduling of VMs and for technical support.</p> <p>Upon cancelling a subscription, a customer may request that their data is deleted. In addition, they can uninstall the GitHub App from their organisation, and deactivate the GitHub OAuth application used to authenticate to the Actuated Dashboard.</p>"},{"location":"faq/#data-security-encryption","title":"Data security &amp; encryption","text":"<p>TLS is enabled on the actuated control plane, the dashboard and on each agent. The TLS certificates have not expired and and have no known issues.</p> <p>Each customer is responsible for hosting their own Servers and installing appropriate firewalls or access control.</p> <p>Each Customer Server requires a unique token which is encrypted using public key cryptography, before being shared with OpenFaaS Ltd. This token is used to authenticate the agent to the control plane.</p> <p>Traffic between the control plane and Customer Server is only made over HTTPS, using TLS encryption and API tokens. In addition, the token required for GitHub Actions is double encrypted with an RSA key pair, so that only the intended agent can decrypt and use it. These tokens are short-lived and expire after 59 minutes.</p> <p>Event data recorded from GitHub Actions is stored and used to deliver quality of service and scheduling. This data is stored on a server managed by DigitalOcean LLC in the United Kingdom. The control plane is hosted with Linode LLC in the United Kingdom.</p> <p>No data is shared with third parties.</p>"},{"location":"faq/#software-development-life-cycle","title":"Software Development Life Cycle","text":"<ul> <li>A Version Control System (VCS) is being Used - GitHub is used by all employees to store code</li> <li>Only Authorized Employees Access Version Control - multiple factor authentication (MFA) is required by all employees</li> <li>Only Authorized Employees Change Code - no changes can be pushed to production without having a pull request approval from senior management</li> <li>Production Code Changes Restricted - Only authorized employees can push orm make changes to production code</li> <li>All changes are documented through pull requests tickets and commit messages</li> <li>Vulnerability management - vulnerability management is provided by GitHub.com. Critical vulnerabilities are remediated in a timely manner</li> </ul> <p>Terminated Employee Access Revoked Within One Business Day - all access to source control management and production systems is revoked within one business day of an employee leaving the company.</p> <p>Access to corporate network, production machines, network devices, and support tools requires a unique ID. This ID is only issued to employees and is revoked upon termination.</p> <p>Policies Cover Employee Confidentiality - OpenFaaS Ltd policies require employees to keep confidential any information they learn while handling customer data.</p>"},{"location":"faq/#contact-information-available-to-customers","title":"Contact Information Available to Customers","text":"<p>OpenFaaS Ltd has provided an email address in a customer-accessible support documentation where support contact information is readily available. Users are encouraged to contact appropriate OpenFaaS Ltd if they become aware of items such as operational or security failures, incidents, system problems, concerns, or other issues/complaints.</p>"},{"location":"faq/#reliability-and-uptime","title":"Reliability and uptime","text":"<p>Authorized users have access to centralised logging endpoints, to query the logs of the Actuated agent installed on Customer Servers, ad-hoc, for the purpose of support and troubleshooting.</p> <p>Authorized users have access to alerts, dashboards and may use this data to improve the service, or to proactively contact customers when there is a suspected issue.</p> <p>Centralised monitoring and metrics gathered from the control plane have a 14-day retention period, after which data is automatically deleted.</p>"},{"location":"install-agent/","title":"Add your first agent to actuated","text":"<p>actuated is split into three parts:</p> <ol> <li>An Actuated Agent (agent) that you run on your own machines or VMs (server), which can launch a VM with a single-use GitHub Actions runner.</li> <li>A VM image launched by the agent, with all the preinstalled software found on a hosted GitHub Actions runner.</li> <li>Our own control plane that talks to GitHub on your behalf, and schedules builds across your fleet of agents.</li> </ol> <p>All we need you to do is to install our agent on one or more servers, then we take care of the rest. We'll even be able to tell you if your server goes offline for any reason.</p> <p>Have you registered your organisation yet?</p> <p>Before you can add an agent, you or your GitHub organisation admin will need to install the: Actuated GitHub App.</p>"},{"location":"install-agent/#pick-your-actuated-servers","title":"Pick your Actuated Servers","text":"<p>Pick your Actuated Servers carefully using our guide: Pick a host for actuated</p>"},{"location":"install-agent/#review-the-end-user-license-agreement-eula","title":"Review the End User License Agreement (EULA)","text":"<p>Make sure you've read the Actuated EULA before registering your organisation with the actuated GitHub App, or starting the agent binary on one of your hosts.</p> <p>If you missed it in the \"Provision a Server\" page, we recommend you use Ubuntu 22.04 as the host operating system on your Server.</p>"},{"location":"install-agent/#install-the-actuated-agent","title":"Install the Actuated Agent","text":"<ol> <li> <p>Install your license for actuated</p> <p>The license is available in the email you received when you purchased your subscription. If someone else bought the subscription, they should be able to forward it to you.</p> <p>Run the following, then paste in your license, hit enter once, then Control + D to save the file.</p> <pre><code>mkdir -p ~/.actuated\ncat &gt; ~/.actuated/LICENSE\n</code></pre> </li> <li> <p>Download the Actuated Agent and installation script to the server</p> <p>Setting up an ARM64 agent? Wherever you see <code>agent</code> in a command, change it to: <code>agent-arm64</code>. So instead of <code>agent keygen</code> you'd run <code>agent-arm64 keygen</code>.</p> <p>Install arkade using the command below, or download it from the releases page.</p> <p>Download the latest agent and install the binary to <code>/usr/local/bin/</code>:</p> <pre><code>(\n# Install arkade\ncurl -sLS https://get.arkade.dev | sudo sh\n\n# Use arkade to extract the agent from its OCI container image\narkade oci install ghcr.io/openfaasltd/actuated-agent:latest --path ./agent\nchmod +x ./agent/agent*\nsudo mv ./agent/agent* /usr/local/bin/\n)\n</code></pre> <p>Run the setup.sh script which will install all the required dependencies like containerd, CNI and Firecracker.</p> <p>For best performance, a dedicated drive, volume or partition is required to store the filesystems for running VMs. If you do not have a volume or extra drive attached, then you can shrink the root partition, and use the resulting free space.</p> <pre><code>(\ncd agent\nVM_DEV=/dev/nvme0n2 sudo -E ./install.sh\n)\n</code></pre> <p>If you do not have additional storage available at this time, the installer will generate a loopback filesystem for you.</p> <pre><code>(\ncd agent\nsudo ./install.sh\n)\n</code></pre> </li> <li> <p>Generate your enrollment file</p> <p>You'll need to create a DNS A or CNAME record for each server you add to actuated, this could be something like <code>server1.example.com</code> for instance.</p> <p>Run the following to create an enrollment file at <code>$HOME/.actuated/agent.yaml</code>:</p> <p>For an Arm server run <code>agent-arm64</code> instead of <code>agent</code></p> <pre><code>agent enroll --url https://server1.example.com\n</code></pre> <p>The enrollment file contains:</p> <ul> <li>The hostname of the server</li> <li>The public key of the agent which we use to encrypt tokens sent to the agent to bootstrap runners to GitHub Actions</li> <li>A unique API token encrypted with our public key, which is used by the control plane to authenticate each message sent to the agent</li> </ul> </li> <li> <p>Configure and start the agent</p> <p>Use the <code>install-service</code> command to configure and install a systemd service to start the actuated agent.</p> <p>The actuated control plane will only communicate with agents exposed over HTTPS to ensure proper encryption is in place. An API token is used in addition with the TLS connection for all requests.</p> <p>Any bootstrap tokens sent to the agent are further encrypted with the agent's public key.</p> <p>For hosts with public IPs, you will need to use the built-in TLS provisioning with Let's Encrypt. For hosts behind a firewall, NAT or in a private datacenter, you can use inlets to create a secure tunnel to the agent.</p> <p>We're considering other models for after the pilot, for instance GitHub's own API has the runner make an outbound connection and uses long-polling.</p> <p>These steps are for hosts with public IP addresses, if you want to use inlets, jump to the end of this step.</p> <p>The easiest way to configure everything is to run as root. The --user flag can be used to run under a custom user account, however sudo access is still required for actuated.</p> <p>For an x86_64 server, run:</p> <pre><code>DOMAIN=agent1.example.com\n\nsudo -E agent install-service \\\n--letsencrypt-domain $DOMAIN \\\n--letsencrypt-email webmaster@$DOMAIN\n</code></pre> <p>For an Arm server, run: </p> <pre><code>DOMAIN=agent1.example.com\n\nsudo -E agent-arm64 install-service \\\n--letsencrypt-domain $DOMAIN \\\n--letsencrypt-email webmaster@$DOMAIN\n</code></pre> <p>Note the different binary name: <code>agent-arm64</code></p> <p>If you need to make changes you can run the command again, or edit <code>/etc/default/actuated</code>.</p> <p>Check the service's status with:</p> <pre><code>sudo systemctl status actuated\nsudo journalctl -u actuated --since today -f\n</code></pre> <p>For an Actuated Agent behind an inlets tunnel, do not include the <code>--letsencrypt-*</code> flags, and instead add <code>--listen-addr \"127.0.0.1:\"</code>. See expose the agent with HTTPS for instructions on how the setup inlets.</p> <p>For example (with inlets):</p> <pre><code>sudo -E agent install-service \\\n--listen-addr \"127.0.0.1:\"\n</code></pre> </li> <li> <p>Check that the control-plane is accessible</p> <pre><code>curl -i https://server1.example.com\n</code></pre> <p>A correct response is a 403.</p> </li> <li> <p>Send us your agent's connection info</p> <p>Share the <code>$HOME/.actuated/agent.yaml</code> file with us so we can add your agent to the actuated control plane.</p> <p>We'll let you know once we've added your agent to actuated and then it's over to you to start running your builds.</p> <p>Once you've run our test build, you need to run the steps for systemd mentioned above.</p> </li> </ol>"},{"location":"install-agent/#next-steps","title":"Next steps","text":"<p>You can now start your first build and see it run on your actuated agent.</p> <p>Start a build on your agent</p> <pre><code>name: ci\n\non: push\n\njobs:\n build-golang:\n-    runs-on: ubuntu-latest\n+    runs-on: actuated-4cpu-16gb\n</code></pre> <p>The amount of RAM and CPU can be picked independently.</p> <p>For Arm servers change the prefix from <code>actuated-</code> to <code>actuated-arm64</code>:</p> <pre><code>name: ci\n\non: push\n\njobs:\n build-golang:\n-    runs-on: ubuntu-latest\n+    runs-on: actuated-arm64-8cpu-32gb\n</code></pre> <p>You can also customise the VM size for each job:</p> <pre><code>name: ci\n\non: push\n\njobs:\n build-golang:\n-    runs-on: actuated\n+    runs-on: actuated-4cpu-8gb\n</code></pre> <p>In a future version of actuated, you'll be able to specify <code>actuated-any</code> if you don't mind whether the job runs on one of your amd64 or arm64 servers.</p>"},{"location":"install-agent/#other-considerations","title":"Other considerations","text":"<p>If you'd like to install a firewall, ufw should be relatively quick to configure.</p> <p>You will need the following ports open:</p> <ul> <li><code>443</code> - the encrypted control plane for actuated</li> <li><code>80</code> - used with Let's Encrypt to obtain a TLS certificate during the HTTP01 challenge</li> <li><code>22</code> - we recommend leaving port 22 open so that you can log into the machine with SSH, if needed. You could also change this to a high or random port</li> </ul> <p>We do not recommend restricting outgoing traffic on the server as this will probably cause you issues with your builds.</p> <p>See also: Troubleshooting your agent</p>"},{"location":"provision-server/","title":"Provision a Server","text":""},{"location":"provision-server/#provision-a-server-for-actuated","title":"Provision a Server for actuated","text":"<p>You'll need to provision a Server which is capable of virtualisation with Linux KVM. Each of your builds will run in an isolated microVM, with its own networking, Kernel and immutable filesystem.</p> <p>We have done extensive research and testing both independently and with our customers. The recommendations on this page are updated regularly. We recommend bare-metal for the best performance, but cloud VMs which support nested virtualisation are also an option.</p> <p>Did you know? Bare-metal servers from European providers are available from 50-150 EUR / mo. Using your own hardware can also be really cost effective.</p> <p>So what makes one server quicker than another?</p> <ul> <li>CPU Clock speed - the base and turbo speeds affect how some builds perform like Go and Rust</li> <li>Core core - The amount of vCPU allocated to a build affects multi-processing</li> <li>RAM and disk space - tune these to your needs to prevent builds slowing down</li> <li>Generation of hardware - hosted runners may use obsolete hardware, you can use the latest generation </li> <li>Network bandwidth - how quickly images, artifacts and caches will be transferred</li> <li>Storage - NVMe is the only viable option for high performance builds</li> <li>Multi-tenancy - are other customers contenting for the same resources, or is the server dedicated to your team?</li> </ul> <p>What Operating System (OS) should I use?</p> <p>The certified Operating System for an Actuated server is: Ubuntu Server 22.04.</p>"},{"location":"provision-server/#how-many-vms-or-jobs-can-a-server-run","title":"How many VMs or jobs can a server run?","text":"<p>Depending on the level of concurrency in your plan, each server will be able to run a set number of jobs. So we suggest dividing the RAM and CPU threads between them. For instance, if your server has 32 threads and 128GB of RAM, you could allocate 6 vCPU and 25 GB of RAM to each job for 5x jobs in parallel, or 4x vCPU and 12GB of RAM for 10x jobs in parallel.</p> <p>In addition, you can also specify vCPU and RAM requirements on a per-job basis by changing the <code>runs-on: actuated</code> label to: <code>runs-on: actuated-2cpu-8gb</code> and so forth. This is useful for when you have a particular jobs which needs a lot of resources like building Kernels, Kubernetes/Docker E2E tests and browser testing.</p>"},{"location":"provision-server/#just-tell-me-what-i-need","title":"Just tell me what I need","text":"<p>For the absolute best value in terms of performance and cost, we recommend the following options from Hetzner's Dedicated range:</p> <ul> <li>x86_64 - Hetzner's A102</li> <li>Arm64 - Hetzner's RX170</li> </ul> <p>Servers on Hetnzer arrive with a \"rescue\" system, use it to install Ubuntu 22.04, and make sure you disable software RAID, so that the two NVMe drives are presented as separate devices. One will run the system, the other will be used for filesystems for all the VMs.</p>"},{"location":"provision-server/#our-research-on-servers-for-actuated","title":"Our research on servers for actuated","text":"<p>Want us to recommend a server?</p> <p>There's a lot of options when it comes to picking a server. On the onboarding call, we can help you find a match for your performance requirements, budget, and vendor preferences.</p>"},{"location":"provision-server/#intelamd","title":"Intel/AMD","text":"<p>1000 USD free credit for bare-metal</p> <p>Equinix Metal have partnered with us to offer 1000 USD of credit for new customers to use on actuated. This will cover your usage for one month using an AMD Epyc server. You can request the discount code after purchasing your actuated subscription.</p> <p>Intel and AMD CPUs can be used interchangeable and are known as <code>amd64</code> or <code>x86_64</code>.</p> <ol> <li> <p>Bare-metal on the cloud (higher cost, convenient, high performance)</p> <p>Bare-metal doesn't have to mean managing hardware in your own physical datacenter. You can deploy machines by API, pay-as-you-go and get the highest performance available.</p> <p>Bear in mind that whilst the cost of bare-metal is higher than VMs, you will be able to pack more builds into them and get better throughput since actuated can schedule builds much more efficiently than GitHub's self-hosted runner.</p> <p>We have seen the best performance from hosts with high clock speeds like the recent generation of AMD processors, combined with local NVMe storage. Rotational drives and SATA SSDs are significantly slower. At the lower end of bare-metal providers, you'll pay 40-50 EUR / mo per host, moving up to 80-150 EUR / mo for NVMe and AMD processors, when you go up to enterprise-grade bare-metal with 10Gbit uplinks, you'll be more in the range of 500-1500 USD / mo.</p> <p>Some providers have a setup fee, a one-month commitment, or they don't have an API/automated way to order machines. This coupled with the low costs and capacity of bare-metal means autoscaling servers is usually unnecessary.</p> <p>There are at least a dozen options for hosted bare-metal servers:</p> <ul> <li>Alibaba Cloud</li> <li>AWS - untenable pricing for bare-metal servers</li> <li>Berry Byte - US region available</li> <li>Cherry Servers</li> <li>Equinix Metal - 500 USD free credit</li> <li>fasthosts</li> <li>Glesys </li> <li>Hetzner - Region: Germany or Finland</li> <li>Ionos - UK based</li> <li>latitude.sh - EU and US region available </li> <li>OVHcloud - EU and US regions available</li> <li>PhoenixNAP - US and EU regions available </li> <li>Scaleway - France region</li> <li>Vultr</li> </ul> <p>You can see a separate list here.</p> <p>A note on Scaleway: Having tested several of Scaleway bare-metal offerings, we do not recommend their current generation of bare-metal due to slow I/O and CPU speeds.</p> <p>Equinix Metal have partnered with us to offer 500 USD of credit for new customers to use on actuated. You'll get the discount code after signing up with us. We've tested their c3.small.x86 and c2.small.x86 machines, and they are very fast, with enterprise-grade networking and support included, with many different regions available.</p> <p>Are you on a budget or looking to cut costs? Both Ionos (UK) and Hetzner (Germany) have excellent value, with NVMe storage very fast AMD CPUs available.</p> <p>Hetzner have a minimum commitment of one month, and most of the time will also charge a one-time setup fee. We recommend their AX-Line with NVMe and ECC RAM - for instance the AX41-NVME, AX52, or AX102. The best machine on offer is the AX161 which also has a fast delivery time.</p> </li> <li> <p>Cloud Virtual Machines (VMs) with nested virtualization (lowest cost, convenient, mid-level performance)</p> <p>This option may not have the raw speed and throughput of a dedicated, bare-metal host, but keeps costs low and is convenient for getting started.</p> <p>We know of at least three providers which have options for nested virtualisation: DigitalOcean, Google Compute Platform (GCP) (new customers get 300 USD free credits from GCP) support nested virtualisation on their Virtual Machines (VMs), and Azure.</p> </li> <li> <p>Bare-metal on-premises (cheap, convenient, high performance)</p> <p>Running bare-metal on-premises is a cost-effective and convenient way to re-use existing hardware investment.</p> <p>The machine could be racked in your server room, under your desk, or in a co-location datacenter.</p> <p>You can use inlets to expose your agent to actuated.</p> <p>Make sure you segment or isolate the agent into its own subnet, VLAN, DMZ, or VPC so that it cannot access the rest of your network. If you are thinking of running an actuated runner at home, we have suggested iptables rules that worked well for our own testing.</p> </li> </ol>"},{"location":"provision-server/#arm","title":"Arm","text":"<p>64-bit Arm is also known as both <code>aarch64</code> and <code>arm64</code>.</p> <p>Arm CPUs are highly efficient when it comes to power consumption and pack in many more cores than the typical x86_64 CPU. This makes them ideal for running many builds in parallel. In typical testing, we've seen Arm builds running under emulation taking 35-45 minutes being reduced to 1-3 minutes total.</p> <p>For Fluent Bit, a build that was failing after 6 hours using QEMU completed in just 4 minutes using actuated and an Ampere Altra server.</p> <ol> <li> <p>Arm on-demand, in the cloud</p> <p>For ARM64, Hetzner provides outstanding value in their RX-Line with 128GB / 256GB RAM coupled with NVMe and 80 cores for around 200 EUR / mo. These are Ampere Altra Servers. There is a minimum commitment of one month, and an initial setup cost per server.</p> <p>We have several customers running Intel/AMD and Arm builds on Hetzner who have been very happy. Stock can take anywhere between hours, days or weeks to be delivered, and could run out, so check their status page before ordering. </p> <p>Glesys have the Ampere Altra Q80-26 available for roughly \u20ac239 / mo. They are a very similar price to Hetzner and are based in Sweden.</p> <p>PhoenixNAP just started to stock the Ampere Altra Q80-30 as of June 2023. These can be bought on a commitment of hourly, monthly or annually with a varying discount. The range was between 600-700 USD / mo.</p> <p>Following on from that, you have the a1.metal instance on AWS with 16 cores and 30GB / RAM for roughly 0.4 USD / hour, and roughly half that cost with a 1x year reservation. The a1.metal is the first generation of Graviton and in our testing with customers came up quite a bit slower than Ampere or Graviton 3. On the plus side, these machines are cheap and if you're already on AWS, it may be easier to start with. GP3 volumes or provisioned concurrency may increase performance over the default of GP2 volumes. Reach out to us for more information.</p> <p>For responsive support, faster uplinks, API-provisioning, per-minute billing and enterprise-grade networking, take a look at the c3.large.arm64 (Ampere Altra) from Equinix Metal. These machines come in at around 2.5 USD / hour, but are packed out with many cores and other benefits. You can usually provision these servers in the Washington DC and Dallas metros. Cloud Native Computing Foundation (CNCF) projects may be able to apply for free credits from Equinix Metal.</p> </li> <li> <p>Arm for on-premises</p> <p>For on-premises ARM64 builds, we recommend the Mac Mini M1 (2020) with 16GB RAM and 512GB storage with Asahi Linux. The M2 is unable to run Linux at this time.</p> <p>Ampere and their partners also offer 1U and 2U servers, along with and desktop-form workstations which can be racked or installed in your office.</p> <p>The Raspberry Pi 4 also works when used with an external NVMe, and in one instance was much faster than using emulation with a Hosted GitHub Runner.</p> </li> <li> <p>Arm VMs with nested virtualisation</p> <p>The current generations of Arm CPUs available from cloud providers do not support KVM, or nested virtualisation, which means you need to pick from the previous two options.</p> <p>There are Arm VMs available on Azure, GCP, and Oracle OCI. We have tested each and since they are based on the same generation of Ampere Altra hardware, we can confirm that they do not have KVM available and will not work for running actuated builds.</p> </li> </ol>"},{"location":"provision-server/#want-to-talk-to-us","title":"Want to talk to us?","text":"<p>Still not sure which option is right for your team? Get in touch with us on the Actuated Slack and we'll help you decide.</p>"},{"location":"provision-server/#next-steps","title":"Next steps","text":"<p>Now that you've created a Server or VM with the recommended Operating System, you'll need to install the actuated agent and get in touch with us, to register it.</p> <ul> <li>Install the Actuated Agent</li> </ul>"},{"location":"register/","title":"Register your GitHub Organisation","text":"<p>Actuated is a managed service, where you plug in your own servers, and we do everything else needed to create and manage self-hosted runners for your GitHub Actions.</p> <p>Plans are paid monthly, without any minimum commitment.</p>"},{"location":"register/#what-youll-need","title":"What you'll need","text":"<ul> <li>A GitHub organisation</li> <li>One or more public or private repositories hosted in the organisation</li> <li>Administrative access to install the actuated GitHub App</li> <li>A company credit-card to pay for your subscription</li> <li>One or more bare-metal servers (we'll recommend the best fit for you during onboarding)</li> </ul>"},{"location":"register/#well-guide-you-through-the-process","title":"We'll guide you through the process","text":"<p>We'll walk you through the onboarding process and answer all your questions, so that you can be up and running straight away.</p> <p>We've now run over 160,000 VMs for commercial teams already, and there's very little for you to do to get started, in most cases, we've seen a 2-3x speed up for <code>x86_64</code> builds by switching one line in a workflow: <code>runs-on: actuated</code>. For <code>Arm</code> builds, native hardware makes a night and day difference.</p>"},{"location":"register/#book-a-call-with-us","title":"Book a call with us","text":"<p>Some engineers hate talking to sales people. You're not alone, and this is not a sales call, or with sales people. Our pricing is public, and paid month to month by corporate card.</p> <p>The purpose of a call is to understand your goals, and help you pick the best server hardware, hosting company, and subscription plan for your usage.</p> <p>Before the call, generate a usage report with our open-source actions-usage tool for at least 7 days, and either send it over via email or share it with us on the call. It'll help us make a better recommendation.</p> <p>Talk to us</p>"},{"location":"register/#install-the-github-app","title":"Install the GitHub App","text":"<p>An administrator from your GitHub organisation will need to install the actuated GitHub App. GitHub Apps provide fine-grained access controls for third-parties integrating with GitHub.</p> <p>Learn more in the FAQ. </p> <p>End User License Agreement (EULA)</p> <p>Make sure you've read the Actuated EULA before registering your organisation with the actuated GitHub App.</p> <ol> <li>Click on the Actuated GitHub App</li> <li>Click Install app</li> <li>Select the organisation you want to install the Actuated app to</li> <li> <p>Install the app on all repositories or select repositories</p> <p></p> </li> <li> <p>Once installed you will will see the permissions and other configuration options for the Actuated GitHub App on your selected account. If you have multiple organisations, you will need to authorise each one.</p> </li> </ol> <p>To remove or update the Actuated GitHub app, navigate to \"Settings\" for your organisation and click on \"GitHub Apps\" in the left sidebar. Select the Actuated from the list and click \"Configure\".</p>"},{"location":"register/#next-steps","title":"Next steps","text":"<p>Now that you've installed the GitHub App, and picked a subscription plan:</p> <ul> <li>Provision a server</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Actuated is in a pilot phase, running builds for participating customers. The core experience is functioning and we are dogfooding it for actuated itself, OpenFaaS Pro and inlets.</p> <p>Our goal with the pilot is to prove that there's market fit for a solution like this, and if so, we'll invest more time in automation, user experience, agent autoscaling, dashboards and other polish.</p> <p>The technology being used is run at huge scale in production by AWS (on Lambda and Fargate) and GitHub (self-hosted runners use the same runner software and control plane).</p> <p>We believe actuated solves a real problem making CI fast, efficient, and multi-arch.</p> <p>If you'd like to try it out for your team, Register interest for the pilot.</p> <p>Shipped</p> <ul> <li> Firecracker MicroVM support for runners</li> <li> Secure builds for both public and private repos</li> <li> Fat VM image to match tooling installed by GitHub Actions</li> <li> KinD support for runner's Kernel</li> <li> K3s support for runner's Kernel</li> <li> ARM64 support, including Raspberry Pi 4B</li> <li> Efficient scheduling of jobs across fleet of agents</li> <li> Samples for K3s/KinD/Matrix builds and OpenFaaS functions</li> <li> Subscription plans delivered by Gumroad</li> <li> API for reviewing connected agents and queue depth</li> <li> Job event auditing for review via API</li> <li> Documentation site with detailed GitHub Actions examples</li> <li> Customer dashboard UI to show connected agents and build queue</li> <li> Official website actuated.dev</li> <li> Remote / automated update of agents via control plane</li> <li> Blog feature on actuated.dev with news, tutorials and updates from our team</li> <li> Performance testing for Ionos &amp; Scaleway for cost effective AMD bare-metal</li> <li> Daily build statistics on your dashboard</li> <li> Docker cache directly on the Actuated Hosts (servers) for much faster builds and avoiding rate-limiting</li> <li> Subscriptions: migration to LemonSqueezy for lower fees, and more payment options</li> <li> Dashboard - animation on all data pages for better feedback when refreshing data</li> <li> Detailed insights across your organisation on usage</li> <li> Detailed insights across your repos</li> <li> Detailed insights by committer</li> <li> Integrated SSH debug for runners within dashboard and CLI</li> <li> At a glance insights for the day's activity so far</li> <li> CLI/API for remote logs of VMs and the actuated agent</li> <li> CLI/API for restarting the agent and rebooting a server</li> <li> OIDC Proxy for OpenFaaS CE/Standard/Enterprise users for keyless login</li> <li> Examples for using S3/Minio running on the server as an actions cache, instead of the default hosted cache within Azure</li> <li> Specify a custom runner size for an individual workflow - i.e. <code>actuated-8cpu-12gb</code></li> <li> Specify <code>actuated-any</code> to run jobs on any available server whether amd64 or arm64, for architecture-agnostic workflows such as npm or for browser testing. </li> </ul> <p>Coming next:</p> <ul> <li> GPU pass-through for ML and AI workloads</li> <li> Support for private, self-hosted GitHub Enterprise Server (GHES) installations</li> <li> Actuated for self-hosted GitLab. Watch the video here</li> </ul> <p>Open for customer contributions:</p> <ul> <li> Examples for setting up an apt/yum mirror for faster builds</li> <li> Example for configuring two different Docker pull through registries instead of just one.</li> </ul> <p>Under consideration:</p> <ul> <li> Custom CA for self-hosted S3, Minio, Docker Registries, apt/yum mirrors, etc.</li> <li> Summary of CPU/RAM/disk consumption of builds</li> <li> Right-sizing of build VMs based upon prior build history</li> <li> Automated agent installation and bootstrap</li> <li> Actuated for Jenkins</li> </ul> <p>Items marked under consideration are awaiting customer interest. Reach out to us if you'd like to see these features sooner.</p> <p>Is there something else you need? If you're already a customer, contact us via the actuated Slack or Register for interest.</p>"},{"location":"test-build/","title":"Run a test build","text":"<p>Now that you've registered your GitHub organisation, created a server, and configured the agent, you're ready for a test build.</p> <p>We recommend you run the following build without changes to confirm that everything is working as expected. After that, you can modify an existing build and start using actuated for your team.</p> <p>The below steps should take less than 10 minutes.</p>"},{"location":"test-build/#create-a-repository-and-workflow","title":"Create a repository and workflow","text":"<p>This build will show you the specs, OS and Kernel name reported by the MicroVM.</p> <p>Note that if you're running on an Arm64 machine, instead of <code>runs-on: actuated</code>, you'll need to specify <code>runs-on: actuated-arm64</code> instead.</p> <ol> <li> <p>Create a test repository and a GitHub Action</p> <p>Create <code>./.github/workflows/ci.yaml</code>:</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\nworkflow_dispatch:\n\njobs:\nspecs:\nname: specs\n# runs-on: actuated-arm64\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v1\n- name: Check specs\nrun: |\n./specs.sh\n</code></pre> <p>Note that the <code>runs-on:</code> field says <code>actuated</code> and not <code>ubuntu-latest</code>. This is how the actuated control plane knows to send this job to your agent.</p> <p>Then add <code>specs.sh</code> to the root of the repository, and remember, that you must run <code>chmod +x specs.sh</code> afterwards to make it executable.</p> <pre><code>#!/bin/bash\n\necho Information on main disk\ndf -h /\n\necho Memory info\nfree -h\n\necho Total CPUs:\necho CPUs: $(nproc)\n\necho CPU Model\ncat /proc/cpuinfo |grep -i \"Model\"|head -n 2\n\necho Kernel and OS info\nuname -a\n\necho Generally, KVM should not be available unless specifically enabled\nif ! [ -e /dev/kvm ]; then\necho \"/dev/kvm does not exist\"\nelse\necho \"/dev/kvm exists\"\nfi\n\necho OS\ncat /etc/os-release\n\necho Egress IP:\ncurl -s -L -S https://checkip.amazonaws.com\n\necho Speed test of Internet\nsudo pip install speedtest-cli\nspeedtest-cli\n\necho Checking Docker\ndocker run alpine:latest cat /etc/os-release\n</code></pre> <p>Don't leave out this step!</p> <pre><code>chmod +x ./specs.sh\n</code></pre> </li> <li> <p>Hit commit, and watch the VM boot up.</p> <p>You'll be able to see the runners registered for your organisation on the Actuated Dashboard along with the build queue and stats for the current day's builds.</p> </li> <li> <p>If you're curious</p> <p>You can view the logs of the agent by logging into one of the Actuated Servers with SSH and running the following commands:</p> <pre><code>sudo journalctl -u actuated -f -o cat\n\n# Just today's logs:\nsudo journalctl -u actuated --since today -o cat\n</code></pre> <p>And each VM writes the logs from its console and the GitHub Actions Runner to <code>/var/log/actuated/</code>.</p> <pre><code>sudo cat /var/log/actuated/*\n</code></pre> </li> </ol> <p>Do you have any questions or comments? Feel free to reach out to us over Slack in the <code>#onboarding</code> channel.</p>"},{"location":"test-build/#enable-actuated-for-an-existing-repository","title":"Enable actuated for an existing repository","text":"<p>To add actuated to an existing repository, simply edit the workflow YAML file and change <code>runs-on:</code> to <code>runs-on: actuated</code> and for Arm builds, change it to: <code>runs-on: actuated-arm64</code>.</p>"},{"location":"test-build/#recommended-enable-a-docker-hub-mirror","title":"Recommended: Enable a Docker Hub mirror","text":"<p>Do you use the Docker Hub in your builds? Any Dockerfile with a <code>FROM</code> that doesn't include a server name will be pulled from <code>docker.io</code>, and there are strict rate-limits for unauthenticated users.</p> <ol> <li> <p>Option 1 - authenticate</p> <p>Run <code>docker login</code> or use the Docker Login Action just before you run Docker build or pull down any images with tooling like KinD 2. Option 2 - use a cache/mirror</p> <p>Use our guide to Set up a registry cache and mirror - this uses less bandwidth and increases the speed of builds where images are already present in the cache.</p> </li> </ol>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#getting-support","title":"Getting support","text":"<p>All customers have access to a public Slack channel for support and collaboration.</p> <p>Enterprise customers may also have an upgraded SLA for support tickets via email and access to a private Slack channel.</p>"},{"location":"troubleshooting/#billing-and-your-plan","title":"Billing and your plan","text":""},{"location":"troubleshooting/#change-your-credit-card","title":"Change your credit card","text":"<p>Sometimes credit card limits or virtual cards are used on a subscription. To change the credit card used for your subscription, click here: My Orders.</p>"},{"location":"troubleshooting/#upgrade-your-plan","title":"Upgrade your plan","text":"<p>If you'd like to upgrade your plan for more concurrent builds, a higher level of support or anything else, you can do so via the Lemon Squeezy dashboard, the additional amount will be applied pro-rata.</p> <p>Update or review your plan</p>"},{"location":"troubleshooting/#the-actuated-dashboard","title":"The Actuated Dashboard","text":"<p>The first port of call should be the Actuated Dashboard where you can check the status of your agents and see the current queue of jobs.</p> <p>For security reasons, an administrator for your GitHub Organisation will need to approve the Actuated Dashboard for access to your organisation before team members will be able to see any data. Send them the link for the dashboard, and have them specifically tick the checkbox for your organisation when logging in for the first time.</p> <p>If you missed this step, have them head over to their Applications Settings page, click \"Authorized OAuth Apps\" and then \"Actuated Dashboard\". On this page, under \"Organization access\" they can click \"Grant\" for each of your organisations registered for actuated.</p> <p></p> <p>How to \"Grant\" access to the Dashboard.</p> <p>Try a direct link here: Actuated Dashboard OAuth App</p>"},{"location":"troubleshooting/#a-job-is-stuck-as-queued","title":"A job is stuck as \"queued\"","text":"<p>If you're using a private repo and the job is queued, let us know on Slack and we'll check the audit database to try and find out why.</p> <p>To remotely check the logs of the actuated service on a host, run the following:</p> <pre><code>actuated-cli agent-logs --owner ORG --host HOST [--age 20m]\n</code></pre> <p>Or you can also check <code>/var/log/actuated/</code> for log files, <code>tail -n 20 /var/log/actuated/*.txt</code> should show you any errors that may have occurred on any of the VM boot-ups or runner registrations. Or check <code>sudo journalctl -u actuated</code> to see what's happening within the actuated service.</p> <p>Since 2022, the main GitHub service and/or Actions has had a high number of partial or full outages.</p> <p>Check the GitHub Status Page to make sure GitHub is fully operational, and bear in mind that there could be an issue, even if it hasn't been shown on that page yet.</p> <p>You can schedule additional VMs to launch, one per queued job with the CLI:</p> <pre><code>actuated-cli repair --org OWNER\n</code></pre> <p>This command should not be run multiple times without contacting support first.</p>"},{"location":"troubleshooting/#you-pull-a-lot-of-large-images-from-the-docker-hub","title":"You pull a lot of large images from the Docker Hub","text":"<p>As much as we like to make our images as small as possible, sometimes we just have to pull down either large artifacts or many smaller ones. It just can't be helped.</p> <p>Since a MicroVM is a completely immutable environment, the pull needs to happen on each build, which is actually a good thing.</p> <p>The pull speed can be dramatically improved by using a registry mirror on each agent:</p> <ul> <li>Example: Set up a registry mirror</li> </ul>"},{"location":"troubleshooting/#you-are-running-into-rate-limits-when-using-container-images-from-the-docker-hub","title":"You are running into rate limits when using container images from the Docker Hub","text":"<p>The Docker Hub implements stringent rate limits of 100 pulls per 6 hours, and 200 pulls per 6 hours if you log in. Pro accounts get an increased limit of 5000 pulls per 6 hours.</p> <p>We've created simple instructions on how to set up a registry mirror to cache images on your Actuated Servers.</p> <ul> <li>Example: Set up a registry mirror</li> </ul>"},{"location":"troubleshooting/#a-job-is-running-out-of-ram-or-needs-more-cores","title":"A job is running out of RAM or needs more cores","text":"<p>If you suspect a job is running out of RAM or would benefit from more vCPU, you can increase the allocation by changing the <code>actuated</code> label, as follows:</p> <pre><code>-runs-on: actuated\n+runs-on: actuated-8cpu-16gb\n</code></pre> <p>You must set both RAM and vCPU at the same time, in the order of CPU (given in a whole number) followed by RAM (specified in GB)</p> <p>For arm64 builds, the format follows the same convention: <code>actuated-arm64-8cpu-16gb</code>.'</p> <p>Bear in mind that if you set the RAM higher than the default, this may result in fewer concurrent VMs being scheduled on a single server.</p> <p>The maximum amount of vCPU that can be set for a single job is 32 vCPU, this is an implementation detail of Firecracker and may change in the future.</p>"},{"location":"troubleshooting/#disk-space-is-running-out-for-a-job","title":"Disk space is running out for a job","text":"<p>The disk space allocated for jobs is 30GB by default, but this value can be increased. Contact the actuated team for instructions on how to do this.</p> <p>A dedicated disk or partition should be allocated for your VMs, if that's not the case, contact us and we'll help you reconfigure the server.</p>"},{"location":"troubleshooting/#your-agent-has-been-offline-or-unavailable-for-a-significant-period-of-time","title":"Your agent has been offline or unavailable for a significant period of time","text":"<p>If your agent has been offline for a significant period of time, then our control plane will have disconnected it from its pool of available agents.</p> <p>Contact us via Slack to have it reinstated.</p>"},{"location":"troubleshooting/#you-need-to-rotate-the-authentication-token-used-for-your-agent","title":"You need to rotate the authentication token used for your agent","text":"<p>There should not be many reasons to rotate this token, however, if something's happened and it's been leaked or an employee has left the company, contact us via email for the update procedure.</p>"},{"location":"troubleshooting/#you-need-to-rotate-your-privatepublic-keypair","title":"You need to rotate your private/public keypair","text":"<p>Your private/public keypair is comparable to an SSH key, although it cannot be used to gain access to your agent via SSH.</p> <p>If you need to rotate it for some reason, please contact us by email as soon as you can.</p>"},{"location":"troubleshooting/#your-builds-are-slower-than-expected","title":"Your builds are slower than expected","text":"<ul> <li>Check free disk space (<code>df -h</code>)</li> <li>Check for unattended updates/upgrades (<code>ps -ef | grep unattended-upgrades</code>) and (<code>ps -ef | grep apt</code>)</li> </ul> <p>If you're using spinning disks, then consider switching to SSDs. If you're already using SSDs, consider using PCIe/NVMe SSDs.</p> <p>Finally, we do have another way to speed up microVMs by attaching another drive or partition to your host. Contact us for more information.</p>"},{"location":"examples/custom-vm-size/","title":"Example: Custom VM sizes","text":"<p>Our team will have configured your servers so that they always launch a pre-defined VM size, this keeps the user experience simple and predictable.</p> <p>However, you can also request a specific VM size with up to 32vCPU and as much RAM as is available in the server. vCPU can be over-committed safely, however over-committing on RAM is not advised because if all of the RAM is required, one of the running VMs may exit or be terminated.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul>"},{"location":"examples/custom-vm-size/#request-a-custom-vm-size","title":"Request a custom VM size","text":"<p>For a custom size just append <code>-cpu-</code> and <code>-gb</code> to the above labels, for example:</p> <p>x86_64 example:</p> <ul> <li><code>actuated-1cpu-2gb</code></li> <li><code>actuated-4cpu-16gb</code></li> </ul> <p>64-bit Arm example:</p> <ul> <li><code>actuated-arm64-4cpu-16gb</code></li> <li><code>actuated-arm64-32cpu-64gb</code></li> </ul> <p>You can change vCPU and RAM independently, there are no set combinations, so you can customise both to whatever you like.</p> <p>The upper limit for vCPU is 32.</p> <p>Create a new file at: <code>.github/workflows/build.yml</code> and commit it to the repository.</p> <pre><code>name: specs\n\non: push\njobs:\nspecs:\nruns-on: actuated-1cpu-2gb\nsteps:\n- name: Print specs\nrun: |\nnproc\nfree -h\n</code></pre> <p>This will allocate 1x vCPU and 2GB of RAM to the VM. To run this same configuration for arm64, change <code>runs-on</code> to <code>actuated-arm64-1cpu-2gb</code>.</p>"},{"location":"examples/docker/","title":"Example: Kubernetes with KinD","text":"<p>Docker CE is preinstalled in the actuated VM image, and will start upon boot-up.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/docker/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workflows/build.yml</code> and commit it to the repository.</p> <p>Try running a container to ping Google for 3 times:</p> <pre><code>name: build\n\non: push\njobs:\nping-google:\nruns-on: actuated-4cpu-16gb\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: Run a ping to Google with Docker\nrun: |\ndocker run --rm -i alpine:latest ping -c 3 google.com\n</code></pre> <p>Build a container with Docker:</p> <pre><code>name: build\n\non: push\njobs:\nbuild-in-docker:\nruns-on: actuated-4cpu-16gb\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: Build inlets-connect using Docker\nrun: |\ngit clone --depth=1 https://github.com/alexellis/inlets-connect\ncd inlets-connect\ndocker build -t inlets-connect .\ndocker images\n</code></pre> <p>To run this on ARM64, just change the actuated prefix from <code>actuated-</code> to <code>actuated-arm64-</code>.</p>"},{"location":"examples/github-actions-cache/","title":"Example: GitHub Actions cache","text":"<p>Jobs on Actuated runners start in a clean VM each time. This means dependencies need to be downloaded and build artifacts or caches rebuilt each time. Caching these files in the actions cache can improve workflow execution time.</p> <p>A lot of the setup actions for package managers have support for caching built-in. See: setup-node, setup-python, etc. They require minimal configuration and will create and restore dependency caches for you.</p> <p>If you have custom workflows that could benefit from caching the cache can be configured manually using the actions/cache.</p> <p>Using the actions cache is not limited to GitHub hosted runners but can be used with self-hosted runners. Workflows using the cache action can be converted to run on Actuated runners. You only need to change <code>runs-on: ubuntu-latest</code> to <code>runs-on: actuated</code>.</p>"},{"location":"examples/github-actions-cache/#use-the-github-actions-cache","title":"Use the GitHub Actions cache","text":"<p>In this short example we will build alexellis/registry-creds. This is a Kubernetes operator that can be used to replicate Kubernetes ImagePullSecrets to all namespaces.</p>"},{"location":"examples/github-actions-cache/#enable-caching-on-a-supported-action","title":"Enable caching on a supported action","text":"<p>Create a new file at: <code>.github/workflows/build.yaml</code> and commit it to the repository.</p> <pre><code>name: build\n\non: push\n\njobs:\nbuild:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v3\nwith:\nrepository: \"alexellis/registry-creds\"\n- name: Setup Golang\nuses: actions/setup-go@v3\nwith:\ngo-version: ~1.19\ncache: true\n- name: Build\nrun: |\nCGO_ENABLED=0 GO111MODULE=on \\\ngo build -ldflags \"-s -w -X main.Release=dev -X main.SHA=dev\" -o controller\n</code></pre> <p>To configure caching with the setup-go action you only need to set the <code>cache</code> input parameter to true.</p> <p>The cache is populated the first time this workflow runs. Running the workflow after this should be significantly faster because dependency files and build outputs are restored from the cache.</p>"},{"location":"examples/github-actions-cache/#manually-configure-caching","title":"Manually configure caching","text":"<p>If there is no setup action for your language that supports caching it can be configured manually.</p> <p>Create a new file at: <code>.github/workflows/build.yaml</code> and commit it to the repository.</p> <pre><code>name: build\n\non: push\n\njobs:\nbuild:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v3\nwith:\nrepository: \"alexellis/registry-creds\"\n- name: Setup Golang\nuses: actions/setup-go@v3\nwith:\ngo-version: ~1.19\ncache: true\n- name: Setup Golang caches\nuses: actions/cache@v3\nwith:\npath: |\n~/.cache/go-build\n~/go/pkg/mod\nkey: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}\nrestore-keys: |\n${{ runner.os }}-go-\n- name: Build\nrun: |\nCGO_ENABLED=0 GO111MODULE=on \\\ngo build -ldflags \"-s -w -X main.Release=dev -X main.SHA=dev\" -o controller\n</code></pre> <p>The setup <code>Setup Golang caches</code> uses the cache action to configure caching.</p> <p>The <code>path</code> parameter is used to set the paths on the runner to cache or restore. The <code>key</code> parameter sets the key used when saving the cache. A hash of the go.sum file is used as part of the cache key.</p>"},{"location":"examples/github-actions-cache/#further-reading","title":"Further reading","text":"<ul> <li>Checkout the list of <code>actions/cache</code> examples to configure caching for different languages and frameworks.</li> <li>See our blog: Make your builds run faster with Caching for GitHub Actions</li> </ul>"},{"location":"examples/k3s/","title":"Example: Kubernetes with k3s","text":"<p>You may need to access Kubernetes within your build. K3s is a for-production, lightweight distribution of Kubernetes that uses fewer resources than upstream. k3sup is a popular tool for installing k3s.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/k3s/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workflows/build.yml</code> and commit it to the repository.</p> <p>Note that it's important to make sure Kubernetes is responsive before performing any commands like running a Pod or installing a helm chart.</p> <pre><code>name: k3sup-tester\n\non: push\njobs:\nk3sup-tester:\nruns-on: actuated-4cpu-16gb\nsteps:\n- name: get arkade\nuses: alexellis/setup-arkade@v1\n- name: get k3sup and kubectl\nuses: alexellis/arkade-get@master\nwith:\nkubectl: latest\nk3sup: latest\n- name: Install K3s with k3sup\nrun: |\nmkdir -p $HOME/.kube/\nk3sup install --local --local-path $HOME/.kube/config\n- name: Wait until nodes ready\nrun: |\nk3sup ready --quiet --kubeconfig $HOME/.kube/config --context default\n- name: Wait until CoreDNS is ready\nrun: |\nkubectl rollout status deploy/coredns -n kube-system --timeout=300s\n- name: Explore nodes\nrun: kubectl get nodes -o wide\n- name: Explore pods\nrun: kubectl get pod -A -o wide\n</code></pre> <p>To run this on ARM64, just change the actuated prefix from <code>actuated-</code> to <code>actuated-arm64-</code>.</p>"},{"location":"examples/kernel/","title":"Example: Test that compute time by compiling a Kernel","text":"<p>Use this sample to test the raw compute speed of your hosts by building a Kernel.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/kernel/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workflows/build.yml</code> and commit it to the repository.</p> <pre><code>name: microvm-kernel\n\non: push\njobs:\nmicrovm-kernel:\nruns-on: actuated\nsteps:\n- name: free RAM\nrun: free -h\n- name: List CPUs\nrun: nproc\n- name: get build toolchain\nrun: |\nsudo apt update -qy\nsudo apt-get install -qy \\\ngit \\\nbuild-essential \\\nkernel-package \\\nfakeroot \\\nlibncurses5-dev \\\nlibssl-dev \\\nccache \\\nbison \\\nflex \\\nlibelf-dev \\\ndwarves\n- name: clone linux\nrun: |\ntime git clone https://github.com/torvalds/linux.git linux.git --depth=1 --branch v5.10\ncd linux.git\ncurl -o .config -s -f https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/resources/guest_configs/microvm-kernel-x86_64-5.10.config\necho \"# CONFIG_KASAN is not set\" &gt;&gt; .config\n- name: make config\nrun: |\ncd linux.git \nmake oldconfig\n- name: Make vmlinux\nrun: |\ncd linux.git\ntime make vmlinux -j$(nproc)\ndu -h ./vmlinux\n</code></pre> <p>When you have a build time, why not change <code>runs-on: actuated</code> to <code>runs-on: ubuntu-latest</code> to compare it to a hosted runner from GitHub?</p> <p>Here's our test, where our own machine built the Kernel 4x faster than a hosted runner:</p> <p></p>"},{"location":"examples/kind/","title":"Example: Kubernetes with KinD","text":"<p>You may need to access Kubernetes within your build. KinD is a popular option, and easy to run in an action.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/kind/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workflows/build.yml</code> and commit it to the repository.</p> <p>Note that it's important to make sure Kubernetes is responsive before performing any commands like running a Pod or installing a helm chart.</p> <pre><code>name: build\n\non: push\njobs:\nstart-kind:\nruns-on: actuated-4cpu-16gb\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: get arkade\nuses: alexellis/setup-arkade@v1\n- name: get kubectl and kubectl\nuses: alexellis/arkade-get@master\nwith:\nkubectl: latest\nkind: latest\n- name: Create a KinD cluster\nrun: |\nmkdir -p $HOME/.kube/\nkind create cluster --wait 300s\n- name: Wait until CoreDNS is ready\nrun: |\nkubectl rollout status deploy/coredns -n kube-system --timeout=300s\n- name: Explore nodes\nrun: kubectl get nodes -o wide\n- name: Explore pods\nrun: kubectl get pod -A -o wide\n- name: Show kubelet logs\nrun: docker exec kind-control-plane journalctl -u kubelet\n</code></pre> <p>To run this on ARM64, just change the actuated prefix from <code>actuated-</code> to <code>actuated-arm64-</code>.</p>"},{"location":"examples/kind/#using-a-registry-mirror-for-kind","title":"Using a registry mirror for KinD","text":"<p>Whilst the instructions for a registry mirror work for Docker, and for buildkit, KinD uses its own containerd configuration, so needs to be configured separately, as required.</p> <p>When using KinD, if you're deploying images which are hosted on the Docker Hub, then you'll probably need to either: authenticate to the Docker Hub, or configure the registry mirror running on your server.</p> <p>Here's an example of how to create a KinD cluster, using a registry mirror for the Docker Hub:</p> <pre><code>#!/bin/bash\n\nkind create cluster --wait 300s --config /dev/stdin &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\ncontainerdConfigPatches:\n- |-\n    [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"]\n    endpoint = [\"http://192.168.128.1:5000\"]\nEOF\n</code></pre> <p>With open source projects, you may need to run the build on GitHub's hosted runners some of the time, in which case, you can use a check whether the mirror is available:</p> <pre><code>curl -f --connect-timeout 0.1 -s http://192.168.128.1:5000/v2/_catalog &amp;&gt; /dev/null\n\nif [ \"$?\" == \"0\" ]\nthen\necho \"Mirror found, configure KinD for the mirror\"\nelse\necho \"Mirror not found, use defaults\"\nfi\n</code></pre> <p>To use authentication instead, create a Kubernetes secret of type <code>docker-registry</code> and then attach it to the default service account of each namespace within your cluster.</p> <p>The OpenFaaS docs show how to do this for private registries, but the same applies for authenticating to the Docker Hub to raise rate-limits.</p> <p>You may also like Alex's alexellis/registry-creds project which will replicate your Docker Hub credentials into each namespace within a cluster, to make sure images are pulled with the correct credentials.</p>"},{"location":"examples/kvm-guest/","title":"Example: Run a KVM guest","text":"<p>It is possible to launch a Virtual Machine (VM) within a GitHub Action. Support for virtualization is not enabled by default for Actuated. The Agent has to be configured to use a custom kernel.</p> <p>There are some prerequisites to enable KVM support:</p> <ul> <li><code>aarch64</code> runners are not supported at the moment.</li> <li>A bare-metal host for the Agent is required.</li> </ul>"},{"location":"examples/kvm-guest/#configure-the-agent","title":"Configure the Agent","text":"<ol> <li> <p>Make sure nested virtualization is enabled on the Agent host.</p> </li> <li> <p>Edit <code>/etc/default/actuated</code> on the Actuated Agent and add the <code>kvm</code> suffix to the <code>AGENT_KERNEL_REF</code> variable:</p> <pre><code>- AGENT_KERNEL_REF=\"ghcr.io/openfaasltd/actuated-kernel:x86_64-latest\"\n+ AGENT_KERNEL_REF=\"ghcr.io/openfaasltd/actuated-kernel:x86_64-kvm-latest\"\n</code></pre> </li> <li> <p>Also add it to the <code>AGENT_IMAGE_REF</code> line:</p> <pre><code>- AGENT_IMAGE_REF=\"ghcr.io/openfaasltd/actuated-ubuntu22.04:x86_64-latest\"\n+ AGENT_IMAGE_REF=\"ghcr.io/openfaasltd/actuated-ubuntu22.04:x86_64-kvm-latest\"\n</code></pre> </li> <li> <p>Restart the Agent to use the new kernel.</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; \\\nsudo systemctl restart actuated\n</code></pre> </li> <li> <p>Run a test build to verify KVM support is enabled in the runner. The specs script from the test build will report whether <code>/dev/kvm</code> is available.</p> </li> </ol>"},{"location":"examples/kvm-guest/#run-a-firecracker-microvm","title":"Run a Firecracker microVM","text":"<p>This example is an adaptation of the Firecracker quickstart guide that we run from within a GitHub Actions workflow.</p> <p>The workflow instals Firecracker, configures and boots a guest VM and then waits 20 seconds before shutting down the VM and exiting the workflow.</p> <ol> <li> <p>Create a new repository and add a workflow file.</p> <p>The workflow file: <code>./.github/workflows/vm-run.yaml</code>:</p> <pre><code>name: vm-run\n\non: push\njobs:\nvm-run:\nruns-on: actuated-4cpu-8gb\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: Install arkade\nuses: alexellis/setup-arkade@v2\n- name: Install firecracker\nrun: sudo arkade system install firecracker\n- name: Run microVM\nrun: sudo ./run-vm.sh\n</code></pre> </li> <li> <p>Add the <code>run-vm.sh</code> script to the root of the repository.</p> <p>Running the script will:</p> <ul> <li>Get the kernel and rootfs for the microVM</li> <li>Start fireckracker and configure the guest kernel and rootfs</li> <li>Start the guest machine</li> <li>Wait for 20 seconds and kill the firecracker process so workflow finishes.</li> </ul> <p>The <code>run-vm.sh</code> script:</p> <pre><code>#!/bin/bash\n\n# Clone the example repo\ngit clone https://github.com/skatolo/nested-firecracker.git\n\n# Run the VM script\n./nested-firecracker/run-vm.sh </code></pre> </li> <li> <p>Hit commit and check the run logs of the workflow. You should find the login prompt of the running microVM in the logs.</p> </li> </ol> <p>The full example is available on GitHub</p> <p>For more examples and use-cases see:</p> <ul> <li>How to run a KVM guest in your GitHub Actions</li> </ul>"},{"location":"examples/matrix-k8s/","title":"Example: Regression test against various Kubernetes versions","text":"<p>This example launches multiple Kubernetes clusters in parallel for regression and end to end testing.</p> <p>In the example, We're testing the CRD for the inlets-operator on versions v1.16 through to v1.25. You could also switch out k3s for KinD, if you prefer.</p> <p>See also: Actuated with KinD</p> <p></p> <p>Launching 10 Kubernetes clusters in parallel across your fleet of Actuated Servers.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/matrix-k8s/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workflows/build.yml</code> and commit it to the repository.</p> <p>Customise both the array \"k3s\" with the versions you need to test and replace the step \"Test crds\" with whatever you need to install such as helm charts.</p> <pre><code>name: k3s-test-matrix\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\n\njobs:\nkubernetes:\nname: k3s-test-${{ matrix.k3s }}\nruns-on: actuated\nstrategy:\nmatrix:\nk3s: [v1.16, v1.17, v1.18, v1.19, v1.20, v1.21, v1.22, v1.23, v1.24, v1.25]\n\nsteps:\n- uses: actions/checkout@v1\n- uses: alexellis/setup-arkade@v2\n- uses: alexellis/arkade-get@master\nwith:\nkubectl: latest\nk3sup: latest\n\n- name: Create Kubernetes ${{ matrix.k3s }} cluster\nrun: |\nmkdir -p $HOME/.kube/\nk3sup install \\\n--local \\\n--k3s-channel ${{ matrix.k3s }} \\\n--local-path $HOME/.kube/config \\\n--merge \\\n--context default\ncat $HOME/.kube/config\n\nk3sup ready --context default\nkubectl config use-context default\n\n# Just an extra test on top.\necho \"Waiting for nodes to be ready ...\"\nkubectl wait --for=condition=Ready nodes --all --timeout=5m\nkubectl get nodes -o wide\n\n- name: Test crds\nrun: |\necho \"Applying CRD\"\nkubectl apply -f https://raw.githubusercontent.com/inlets/inlets-operator/master/artifacts/crds/inlets.inlets.dev_tunnels.yaml\n</code></pre> <p>The matrix will cause a new VM to be launched for each item in the \"k3s\" array.</p>"},{"location":"examples/matrix/","title":"Example: matrix-build - run a VM per each job in a matrix","text":"<p>Use this sample to test launching multiple VMs in parallel.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code> including Raspberry Pi 4</li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/matrix/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new file at: <code>.github/workflows/build.yml</code> and commit it to the repository.</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\n\njobs:\narkade-e2e:\nname: arkade-e2e\nruns-on: actuated\nstrategy:\nmatrix:\napps: [run-job,k3sup,arkade,kubectl,faas-cli]\nsteps:\n- name: Get arkade\nrun: |\ncurl -sLS https://get.arkade.dev | sudo sh\n- name: Download app\nrun: |\necho ${{ matrix.apps }}\narkade get ${{ matrix.apps }}\nfile /home/runner/.arkade/bin/${{ matrix.apps }}\n</code></pre> <p>The matrix will cause a new VM to be launched for each item in the \"apps\" array.</p>"},{"location":"examples/multiarch-buildx/","title":"Example: Multi-arch with buildx","text":"<p>A multi-arch or multi-platform container is effectively where you build the same container image for multiple different Operating Systems or CPU architectures, and link them together under a single name.</p> <p>So you may publish an image named: <code>ghcr.io/inlets-operator/latest</code>, but when this image is fetched by a user, a manifest file is downloaded, which directs the user to the appropriate image for their architecture.</p> <p>If you'd like to see what these look like, run the following with arkade:</p> <pre><code>arkade get crane\n\ncrane manifest ghcr.io/inlets/inlets-operator:latest\n</code></pre> <p>You'll see a manifests array, with a platform section for each image:</p> <pre><code>{\n\"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n\"manifests\": [\n{\n\"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n\"digest\": \"sha256:bae8025e080d05f1db0e337daae54016ada179152e44613bf3f8c4243ad939df\",\n\"platform\": {\n\"architecture\": \"amd64\",\n\"os\": \"linux\"\n}\n},\n{\n\"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n\"digest\": \"sha256:3ddc045e2655f06653fc36ac88d1d85e0f077c111a3d1abf01d05e6bbc79c89f\",\n\"platform\": {\n\"architecture\": \"arm64\",\n\"os\": \"linux\"\n}\n}\n]\n}\n</code></pre>"},{"location":"examples/multiarch-buildx/#try-an-example","title":"Try an example","text":"<p>This example is taken from the Open Source inlets-operator.</p> <p>It builds a container image containing a Go binary and uses a Dockerfile in the root of the repository. All of the images and corresponding manifest are published to GitHub's Container Registry (GHCR). The action itself is able to authenticate to GHCR using a built-in, short-lived token. This is dependent on the \"permissions\" section and \"packages: write\" being set.</p> <p>View publish.yaml, adapted for actuated:</p> <pre><code>name: publish\n\non:\n push:\n    tags:\n      - '*'\n\njobs:\n publish:\n+    permissions:\n+      packages: write\n\n-   runs-on: ubuntu-latest\n+   runs-on: actuated\n   steps:\n      - uses: actions/checkout@master\n        with:\n          fetch-depth: 1\n\n+     - name: Setup mirror\n+       uses: self-actuated/hub-mirror@master\n     - name: Get TAG\n        id: get_tag\n        run: echo TAG=${GITHUB_REF#refs/tags/} &gt;&gt; $GITHUB_ENV\n      - name: Get Repo Owner\n        id: get_repo_owner\n        run: echo \"REPO_OWNER=$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')\" &gt; $GITHUB_ENV\n\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v2\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n      - name: Login to container Registry\n        uses: docker/login-action@v2\n        with:\n          username: ${{ github.repository_owner }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n          registry: ghcr.io\n\n      - name: Release build\n        id: release_build\n        uses: docker/build-push-action@v3\n        with:\n          outputs: \"type=registry,push=true\"\n          platforms: linux/amd64,linux/arm/v6,linux/arm64\n          build-args: |\n            Version=${{  env.TAG }}\n            GitCommit=${{ github.sha }}\n          tags: |\n            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ github.sha }}\n            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:${{ env.TAG }}\n            ghcr.io/${{ env.REPO_OWNER }}/inlets-operator:latest\n</code></pre> <p>You'll see that we added a <code>Setup mirror</code> step, this explained in the Registry Mirror example</p> <p>The <code>docker/setup-qemu-action@v2</code> step is responsible for setting up QEMU, which is used to emulate the different CPU architectures.</p> <p>The <code>docker/build-push-action@v3</code> step is responsible for passing in a number of platform combinations such as: <code>linux/amd64</code> for cloud, <code>linux/arm64</code> for Arm servers and <code>linux/arm/v6</code> for Raspberry Pi.</p> <p>Within the Dockerfile, we needed to make a couple of changes.</p> <p>You can pick to run the step in either the BUILDPLATFORM or TARGETPLATFORM. The BUILDPLATFORM is the native architecture and platform of the machine performing the build, this is usually amd64. The TARGETPLATFORM is important for the final step of the build, and will be injected based upon one each of the platforms you have specified in the step.</p> <pre><code>- FROM golang:1.18 as builder\n+ FROM --platform=${BUILDPLATFORM:-linux/amd64} golang:1.18 as builder\n</code></pre> <p>For Go specifically, we also updated the <code>go build</code> command to tell Go to use cross-compilation based upon the TARGETOS and TARGETARCH environment variables, which are populated by Docker.</p> <pre><code>GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o inlets-operator\n</code></pre> <p>Learn more in the Docker Documentation: Multi-platform images</p>"},{"location":"examples/multiarch-buildx/#is-it-slow-to-build-for-arm","title":"Is it slow to build for Arm?","text":"<p>Using QEMU can be slow at times, especially when building an image for Arm using a hosted GitHub Runner.</p> <p>We found that we could increase an Open Source project's build time by 22x - from ~ 36 minutes to 1 minute 26 seconds.</p> <p>See also How to make GitHub Actions 22x faster with bare-metal Arm</p> <p>To build a separate image for Arm on an Arm runner, and one for amd64, you could use a matrix build.</p>"},{"location":"examples/multiarch-buildx/#need-a-hand-with-github-actions","title":"Need a hand with GitHub Actions?","text":"<p>Check your plan to see if access to Slack is included, if so, you can contact us on Slack for help and guidance.</p>"},{"location":"examples/openfaas-helm/","title":"Example: Publish an OpenFaaS function","text":"<p>This example will create a Kubernetes cluster using KinD, deploy OpenFaaS using Helm, deploy a function, then invoke the function. There are some additional checks for readiness for Kubernetes and the OpenFaaS gateway.</p> <p>You can adapt this example for any other Helm charts you may have for E2E testing.</p> <p>We also recommend considering arkade for installing CLIs and common Helm charts for testing.</p> <p>Docker CE is preinstalled in the actuated VM image, and will start upon boot-up.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/openfaas-helm/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a new GitHub repository in your organisation.</p> <p>Add: <code>.github/workflows/e2e.yaml</code></p> <pre><code>name: e2e\n\non:\npush:\nbranches:\n- '*'\npull_request:\nbranches:\n- '*'\n\npermissions:\nactions: read\ncontents: read\n\njobs:\ne2e:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: get arkade\nuses: alexellis/setup-arkade@v1\n- name: get kubectl and kubectl\nuses: alexellis/arkade-get@master\nwith:\nkubectl: latest\nkind: latest\nfaas-cli: latest\n- name: Install Kubernetes KinD\nrun: |\nmkdir -p $HOME/.kube/\nkind create cluster --wait 300s\n- name: Add Helm chart, update repos and apply namespaces\nrun: |\nkubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml\nhelm repo add openfaas https://openfaas.github.io/faas-netes/\nhelm repo update\n- name: Install the Community Edition (CE)\nrun: |\nhelm repo update \\\n&amp;&amp; helm upgrade openfaas --install openfaas/openfaas \\\n--namespace openfaas  \\\n--set functionNamespace=openfaas-fn \\\n--set generateBasicAuth=true\n- name: Wait until OpenFaaS is ready\nrun: |\nkubectl rollout status -n openfaas deploy/prometheus --timeout 5m\nkubectl rollout status -n openfaas deploy/gateway --timeout 5m\n- name: Port forward the gateway\nrun: |\nkubectl port-forward -n openfaas svc/gateway 8080:8080 &amp;\n\nattempts=0\nmax=10\n\nuntil $(curl --output /dev/null --silent --fail http://127.0.0.1:8080/healthz ); do\nif [ ${attempts} -eq ${max} ]; then\necho \"Max attempts reached $max waiting for gateway's health endpoint\"\nexit 1\nfi\n\nprintf '.'\nattempts=$(($attempts+1))\nsleep 1\ndone\n- name: Login to OpenFaaS gateway and deploy a function\nrun: |\nPASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=\"{.data.basic-auth-password}\" | base64 --decode; echo)\necho -n $PASSWORD | faas-cli login --username admin --password-stdin \n\nfaas-cli store deploy env\n\nfaas-cli invoke env &lt;&lt;&lt; \"\"\n\ncurl -s -f -i http://127.0.0.1:8080/function/env\n\nfaas-cli invoke --async env &lt;&lt;&lt; \"\"\n\nkubectl logs -n openfaas deploy/queue-worker\n\nfaas-cli describe env\n</code></pre> <p>If you'd like to deploy the function, check out a more comprehensive example of how to log in and deploy in Serverless For Everyone Else</p>"},{"location":"examples/openfaas-publish/","title":"Example: Publish an OpenFaaS function","text":"<p>This example will publish an OpenFaaS function to GitHub's Container Registry (GHCR).</p> <ul> <li>The example uses Docker's buildx and QEMU for a multi-arch build</li> <li>Dynamic variables to inject the SHA and OWNER name from the repo</li> <li>Uses the token that GitHub assigns to the action to publish the containers.</li> </ul> <p>You can also run this example on GitHub's own hosted runners.</p> <p>Docker CE is preinstalled in the actuated VM image, and will start upon boot-up.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/openfaas-publish/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>For alexellis' repository called alexellis/autoscaling-functions, then check out the <code>.github/workflows/publish.yml</code> file:</p> <ul> <li>The \"Setup QEMU\" and \"Set up Docker Buildx\" steps configure the builder to produce a multi-arch image.</li> <li>The \"OWNER\" variable means this action can be run on any organisation without having to hard-code a username for GHCR.</li> <li>Only the bcrypt function is being built with the <code>--filter</code> command added, remove it to build all functions in the stack.yml.</li> <li><code>--platforms linux/amd64,linux/arm64,linux/arm/v7</code> will build for regular Intel/AMD machines, 64-bit Arm and 32-bit Arm i.e. Raspberry Pi, most users can reduce this list to just \"linux/amd64\" for a speed improvement</li> </ul> <p>Make sure you edit <code>runs-on:</code> and set it to <code>runs-on: actuated</code></p> <pre><code>name: publish\n\non:\npush:\nbranches:\n- '*'\npull_request:\nbranches:\n- '*'\n\npermissions:\nactions: read\nchecks: write\ncontents: read\npackages: write\n\njobs:\npublish:\nruns-on: actuated\nsteps:\n- uses: actions/checkout@master\nwith:\nfetch-depth: 1\n- name: Get faas-cli\nrun: curl -sLSf https://cli.openfaas.com | sudo sh\n- name: Pull custom templates from stack.yml\nrun: faas-cli template pull stack\n- name: Set up QEMU\nuses: docker/setup-qemu-action@v1\n- name: Set up Docker Buildx\nuses: docker/setup-buildx-action@v1\n- name: Get TAG\nid: get_tag\nrun: echo ::set-output name=TAG::latest-dev\n- name: Get Repo Owner\nid: get_repo_owner\nrun: &gt;\necho ::set-output name=repo_owner::$(echo ${{ github.repository_owner }} |\ntr '[:upper:]' '[:lower:]')\n- name: Docker Login\nrun: &gt; echo ${{secrets.GITHUB_TOKEN}} | \ndocker login ghcr.io --username \n${{ steps.get_repo_owner.outputs.repo_owner }} \n--password-stdin\n- name: Publish functions\nrun: &gt;\nOWNER=\"${{ steps.get_repo_owner.outputs.repo_owner }}\" \nTAG=\"latest\"\nfaas-cli publish\n--extra-tag ${{ github.sha }}\n--build-arg GO111MODULE=on\n--platforms linux/amd64,linux/arm64,linux/arm/v7\n--filter bcrypt\n</code></pre> <p>If you'd like to deploy the function, check out a more comprehensive example of how to log in and deploy in Serverless For Everyone Else</p>"},{"location":"examples/system-info/","title":"Example: Get system information about your microVM","text":"<p>This sample reveals system information about your runner.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"examples/system-info/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>Create a specs.sh file:</p> <pre><code>#!/bin/bash\n\necho Hostname: $(hostname)\n\necho whoami: $(whoami)\n\necho Information on main disk\ndf -h /\n\necho Memory info\nfree -h\n\necho Total CPUs:\necho CPUs: $(nproc)\n\necho CPU Model\ncat /proc/cpuinfo |grep \"model name\"\n\necho Kernel and OS info\nuname -a\n\nif ! [ -e /dev/kvm ]; then\necho \"/dev/kvm does not exist\"\nelse\necho \"/dev/kvm exists\"\nfi\n\necho OS info: $(cat /etc/os-release)\n\necho PATH: ${PATH}\n\necho Egress IP:\ncurl -s -L -S https://checkip.amazonaws.com\n</code></pre> <p>Create a new file at: <code>.github/workflows/build.yml</code> and commit it to the repository.</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n\njobs:\nspecs:\nname: specs\nruns-on: actuated\nsteps:\n- uses: actions/checkout@v1\n- name: Check specs\nrun: |\n./specs.sh\n</code></pre> <p>Note how the hostname changes every time the job is run.</p>"},{"location":"tasks/cli/","title":"Actuated CLI","text":"<p>Monitor Actuated runners and jobs from the command line.</p>"},{"location":"tasks/cli/#installation","title":"Installation","text":"<p>Download and installation instruction are available via the actuated-dashboard</p> <p>You'll need to run <code>actuated-cli auth</code> first, so that you can get a Personal Access Token with the appropriate scopes from GitHub.</p>"},{"location":"tasks/cli/#view-queued-jobs","title":"View queued jobs","text":"<pre><code>actuated-cli jobs \\\nactuated-samples\n</code></pre>"},{"location":"tasks/cli/#view-runners-for-organization","title":"View runners for organization","text":"<pre><code>actuated-cli runners \\\nactuated-samples\n</code></pre>"},{"location":"tasks/cli/#view-ssh-sessions-available","title":"View SSH sessions available:","text":"<pre><code>actuated-cli ssh ls\n</code></pre> <p>Hosts are ordered by the connected time.</p> <pre><code>| NO  |   ACTOR   |                 HOSTNAME                 |  RX   |  TX   | CONNECTED |\n|-----|-----------|------------------------------------------|-------|-------|-----------|\n|   1 | alexellis | 6aafd53144e2f00ef5cd2c16681eeab4712561a6 | 13679 | 10371 | 6m4s      |\n|   2 | alexellis | fv-az268-245                             | 23124 | 13828 | 12m2s     |\n</code></pre>"},{"location":"tasks/cli/#connect-to-an-ssh-session","title":"Connect to an SSH session","text":"<p>Connect to the first available session from your account:</p> <pre><code>actuated-cli ssh connect\n</code></pre> <p>Connected to the second session in the list:</p> <pre><code>actuated-cli ssh connect 2\n</code></pre> <p>Connect to a specific session by hostname:</p> <pre><code>actuated-cli ssh connect runner1\n</code></pre> <p>Connect to a specific session with a host prefix:</p> <pre><code>actuated-cli ssh connect 6aafd\n</code></pre>"},{"location":"tasks/cli/#check-the-logs-of-vms","title":"Check the logs of VMs","text":"<p>View the serial console and systemd output of the VMs launched on a specific server.</p> <ul> <li>Check for timeouts with GitHub's control-plane</li> <li>View output from the GitHub runner binary</li> <li>See boot-up messages</li> <li>Check for errors if the GitHub Runner binary is out of date</li> </ul> <pre><code>actuated-cli logs \\\n--owner actuated-samples \\\n--age 15m \\\nrunner1\n</code></pre> <p>The age is specified as a Go duration i.e. <code>60m</code> or <code>24h</code>.</p> <p>You can also get the logs for a specific runner by using the <code>--id</code> flag.</p> <pre><code>actuated-cli logs \\\n--owner actuated-samples \\\n--id ea5c285282620927689d90af3cfa3be2d5e2d004\n    runner1\n</code></pre>"},{"location":"tasks/cli/#check-the-logs-of-the-actuated-agent-service","title":"Check the logs of the actuated agent service","text":"<p>Show the logs of the actuated agent binary running on your server.</p> <p>View VM launch times, etc.</p> <pre><code>actuated-cli agent-logs \\\n--owner actuated-samples \\\n--age 60m \\\nrunner1\n</code></pre>"},{"location":"tasks/cli/#schedule-a-repair-to-re-queue-jobs","title":"Schedule a repair to re-queue jobs","text":"<p>If a job has been retried for 30 minutes, without a runner to take it, it'll be taken off the queue. This command will re-queue all jobs that are in a \"queued\" state.</p> <p>Run sparingly because it will launch one VM per job queued.</p> <pre><code>actuated-cli repair \\\nactuated-samples\n</code></pre>"},{"location":"tasks/cli/#rescue-a-remote-server","title":"Rescue a remote server","text":"<p>Restart the agent by sending a <code>kill -9</code> signal:</p> <pre><code>actuated-cli restart \\\n--owner actuated-samples \\\nrunner1\n</code></pre> <p>Any inflight VMs will be killed, see also: <code>actuated-cli update --force</code></p> <p>Reboot the machine, if in an unrecoverable position:</p> <pre><code>actuated-cli restart \\\n--owner actuated-samples \\\n--reboot\n    runner1\n</code></pre> <p>Use with caution, since this may not perform a safe and clean shutdown.</p>"},{"location":"tasks/cli/#json-mode","title":"JSON mode","text":"<p>Add <code>--json</code> to any command to get JSON output for scripting.</p> <p>API rate limits apply, so do not run the CLI within a loop or <code>watch</code> command.</p>"},{"location":"tasks/cli/#help-support","title":"Help &amp; support","text":"<p>Reach out to our team on Slack</p>"},{"location":"tasks/debug-ssh/","title":"Example: Debug a job with SSH","text":"<p>If your it's included within your actuated plan, then you can get a shell into any self-hosted runner - including GitHub's own hosted runners.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code></li> </ul> <p>Use a private repository if you're not using actuated yet</p> <p>GitHub recommends using a private repository with self-hosted runners because changes can be left over from a previous run, even when using Actions Runtime Controller. Actuated uses an ephemeral VM with an immutable image, so can be used on both public and private repos. Learn why in the FAQ.</p>"},{"location":"tasks/debug-ssh/#try-out-the-action-on-your-agent","title":"Try out the action on your agent","text":"<p>You'll need to add the <code>id_token: write</code> permission to your workflow to use this action. It allows the action to authenticate with the SSH gateway using an GitHub Actions OIDC token.</p> <p>Create a <code>.github/workflows/workflow.yaml</code> file</p> <pre><code>name: connect\n\non:\npush:\nbranches:\n- master\n- main\nworkflow_dispatch:\n\npermissions:\nid-token: write\ncontents: read\nactions: read\n\njobs:\nconnect:\nname: connect\nruns-on: actuated\nsteps:\n- uses: self-actuated/connect-ssh@master\n</code></pre> <p>Next, trigger a build via the workflow_dispatch event or a git push to the master branch.</p> <p>Open <code>https://$SSH_GATEWAY/list</code> in your browser and look for your session, you can log in using the SSH command outputted for you.</p> <p>Alternatively, you can view your own SSH sessions from the actuated dashboard.</p> <p>Whenever you have a build that you just can't figure out - or if you want to explore the runner and tune it up to your needs, then you can simply add <code>- uses: self-actuated/connect-ssh@master</code> where you want to pause the build.</p> <p>To release the session run <code>unblock</code> or <code>sudo reboot</code> from the SSH session.</p> <p>Watch a demo:</p>"},{"location":"tasks/local-github-cache/","title":"Run a local GitHub Cache","text":"<p>The cache for GitHub Actions can speed up CI/CD pipelines. Hosted runners are placed close to the cache which means the latency is very low. Self-hosted runners can also make good use of this cache. Just like caching container images on the host in (a registry mirror)[/tasks/registry-mirror/], you can also get a speed boost over the hosted cache by running your own cache directly on the host.</p> <p>To improve cache speeds with Actuated runners you can run a self-hosted S3 server and switch out the official actions/cache@v3 with tespkg/actions-cache@v1. The tespkg/actions-cache@v1 can target S3 instead of the proprietary GitHub cache.</p> <p>You can run the cache on every actuated server for the speed of communicating over a loopback network, or you can run it on a single dedicated server that's placed in the same region as the actuated servers, which will still be very quick.</p> <p>Note that if you have multiple actuated hosts consider running a single dedicated server for the cache. Subsequent jobs can be scheduled to different hosts so there is no guarantee the cache is populated when running a cache on every actuated server.</p>"},{"location":"tasks/local-github-cache/#set-up-an-s3-cache","title":"Set up an S3 cache","text":"<p>There are a couple of options to run a self-hosted S3 server, most notably Seaweedfs and Minio.</p> <p>This guide will cover the setup of Seaweedfs but any S3 compatible service will work in a very similar way.</p>"},{"location":"tasks/local-github-cache/#install-seaweedfs","title":"Install Seaweedfs","text":"<p>Seaweedfs is distributed as a static Go binary, so it can be installed with arkade, or from the GitHub releases page.</p> <pre><code>arkade get seaweedfs\nsudo mv ~/.arkade/bin/seaweedfs /usr/local/bin\n</code></pre> <p>Define a secret key and access key to be used from the CI jobs in the <code>/etc/seaweedfs/s3.conf</code> file.</p> <p>Generate a secret key: <code>openssl rand -hex 16 &gt; secret_key</code></p> <pre><code>export ACCESS_KEY=\"\" # Replace with your access key\nexport SECRET_KEY=\"$(cat ~/secret_key)\"\n\ncat &gt;&gt; /tmp/s3.conf &lt;&lt;EOF\n{\n  \"identities\": [\n    {\n      \"name\": \"actuated\",\n      \"credentials\": [\n        {\n          \"accessKey\": \"$ACCESS_KEY\",\n          \"secretKey\": \"$SECRET_KEY\"\n        }\n      ],\n      \"actions\": [\n        \"Admin\",\n        \"Read\",\n        \"List\",\n        \"Tagging\",\n        \"Write\"\n      ]\n    }\n  ]\n}\nEOF\n\nmkldir -p /etc/seaweedfs\nsudo mv /tmp/s3.conf /etc/seaweedfs/s3.conf\n</code></pre> <p>Install and start Seaweedfs with a systemd unit file:</p> <pre><code>(\ncat &gt;&gt; /tmp/seaweedfs.service &lt;&lt;EOF\n[Unit]\nDescription=SeaweedFS\nAfter=network.target\n\n[Service]\nUser=root\nExecStart=/usr/local/bin/seaweedfs server -ip=192.168.128.1 -volume.max=0 -volume.fileSizeLimitMB=2048 -dir=/home/runner-cache -s3 -s3.config=/etc/seaweedfs/s3.conf\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nmkdir -p /home/runner-cache\nsudo mv /tmp/seaweedfs.service /etc/systemd/system/seaweedfs.service\nsudo systemctl daemon-reload\nsudo systemctl enable seaweedfs --now\n)\n</code></pre> <p>We have set <code>-volume.max=0 -volume.fileSizeLimitMB=2048</code> to minimize the amount of space used and to allow large zip files of up to 2GB, but you can change this to suit your needs.  See <code>seaweedfs server --help</code> for more options.</p> <p>The <code>ip</code> only needs to be set to <code>192.168.128.1</code> if you are running the cache directly on the agent host. If you set up the cache to be accessible by multiple Actuated runner hosts use the appropriate interface IP address.</p> <p>Check the status with:</p> <pre><code>sudo journalctl -u seaweedfs -f\n</code></pre>"},{"location":"tasks/local-github-cache/#use-the-self-hosted-cache","title":"Use the self-hosted cache","text":"<p>To start using the local cache you will need to replace <code>actions/cache@v3</code> with <code>tespkg/actions-cache@v1</code> and add <code>tespkg/actions-cache</code> specific properties in addition to the <code>actions/cache</code> properties in your cache steps.</p> <p>Some actions like setup-node, setup-python, etc come with build-in support for the GitHub actions cache. They are not directly compatible with the self-hosted S3 cache and you will need to configure caching manually.</p> <p>This is an example to manually configure caching for go:</p> <pre><code>name: build\n\non: push\n\njobs:\nbuild:\nruns-on: actuated-4cpu-8gb\nsteps:\n- name: Setup Golang\nuses: actions/setup-go@v3\nwith:\ngo-version: ~1.21\ncache: false\n- name: Setup Golang caches\nuses: tespkg/actions-cache@v1\nwith:\nendpoint: \"192.168.128.1\"\nport: 8333\ninsecure: true\naccessKey: ${{ secrets.ACTIONS_CACHE_ACCESS_KEY }}\nsecretKey: ${{ secrets.ACTIONS_CACHE_SECRET_KEY }}\nbucket: actuated-runners\nregion: local\nuse-fallback: true\n\n# actions/cache compatible properties: https://github.com/actions/cache\npath: |\n~/.cache/go-build\n~/go/pkg/mod\nkey: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}\nrestore-keys: |\n${{ runner.os }}-go-\n</code></pre> <p><code>tespkg/actions-cache</code> specific properties:</p> <ul> <li><code>use-fallback</code> - option means that if Seaweedfs is not installed on the host, or is inaccessible, the action will fall back to using the GitHub cache.</li> <li><code>bucket</code> - the name of the bucket to use in Seaweedfs</li> <li><code>region</code> - the bucket region - use <code>local</code> when running your own S3 cache locally. </li> <li><code>accessKey</code> and <code>secretKey</code> -  the credentials to use to access the bucket - we'd recommend using an organisation-level secret for this.</li> <li><code>insecure</code> - use http instead of https. You may want to create a self-signed certificate for the S3 service and set <code>insecure: false</code> to ensure that the connection is encrypted. If you're running builds within private repositories, tampering is unlikely.</li> </ul> <p>Checkout the list of <code>actions/cache</code> examples to configure caching for different languages and frameworks. Remember to replace <code>actions/cache@v3</code> with <code>tespkg/actions-cache@v1</code> and add the additional properties mentioned above.</p>"},{"location":"tasks/local-github-cache/#caching-the-git-checkout","title":"Caching the git checkout","text":"<p>Caching the git checkout can save a lot of time especially for large repos.</p> <pre><code>jobs:\nbuild:\nruns-on: actuated-4cpu-8gb\nsteps:\n- name: \"Set current date as env variable\"\nshell: bash\nrun: |\necho \"CHECKOUT_DATE=$(date +'%V-%Y')\" &gt;&gt; $GITHUB_ENV\nid: date\n- uses: tespkg/actions-cache@v1\nwith:\nendpoint: \"192.168.128.1\"\nport: 8333\ninsecure: true\naccessKey: ${{ secrets.ACTIONS_CACHE_ACCESS_KEY }}\nsecretKey: ${{ secrets.ACTIONS_CACHE_SECRET_KEY }}\nbucket: actuated-runners\nregion: local\nuse-fallback: true\npath: ./.git\nkey: ${{ runner.os }}-checkout-${{ env.CHECKOUT_DATE }}\nrestore-keys: |\n${{ runner.os }}-checkout-\n</code></pre> <p>The cache key uses a week-year format, rather than a SHA. Why? Because a SHA would change on every build, meaning that a save and load would be performed on every build, using up more space and slowing things down. In this example, there's only 52 cache entries per year.</p>"},{"location":"tasks/local-github-cache/#caching-node_modules-with-pnpm","title":"Caching node_modules with pnpm","text":"<p>For Node.js projects, the node_modules folder and yarn cache can become huge and take a long time to download. Switching to a local S3 cache can help bring that time down.</p> <p>This example uses pnpm, a fast, disk space efficient replacement for npm and yarn.</p> <pre><code>jobs:\nbuild:\nruns-on: actuated-4cpu-8gb\nsteps:\n- name: Install PNPM\nuses: pnpm/action-setup@v2\nwith:\nrun_install: |\n- args: [--global, node-gyp]\n\n- name: Get pnpm store directory\nid: pnpm-cache\nshell: bash\nrun: |\necho \"STORE_PATH=$(pnpm store path)\" &gt;&gt; $GITHUB_OUTPUT\n\n- uses: tespkg/actions-cache@v1\nwith:\nendpoint: \"192.168.128.1\"\nport: 8333\ninsecure: true\naccessKey: ${{ secrets.ACTIONS_CACHE_ACCESS_KEY }}\nsecretKey: ${{ secrets.ACTIONS_CACHE_SECRET_KEY }}\nbucket: actuated-runners\nregion: local\nuse-fallback: true\npath:\n${{ steps.pnpm-cache.outputs.STORE_PATH }}\n~/.cache\n.cache\nkey: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}\nrestore-keys: |\n${{ runner.os }}-pnpm-store-\n\n- name: Install dependencies\nshell: bash\nrun: |\npnpm install --frozen-lockfile --prefer-offline\n</code></pre>"},{"location":"tasks/local-github-cache/#further-reading","title":"Further reading","text":"<ul> <li>From our blog: Fixing the cache latency for self-hosted GitHub Actions</li> <li>A primer on using the GitHub Actions cache: Using caching in builds</li> </ul>"},{"location":"tasks/monitoring/","title":"Monitoring","text":"<p>Our team monitors actuated around the clock, on your behalf</p> <p>The actuated team proactively monitors your servers and build queue for issues. We remediate them on your behalf and for anything cannot be fixed remotely, we'll be in touch via Slack or email.</p> <p>The actuated CLI should be used for support, to query the agent's logs, or the logs of individual VMs.</p> <p>If you would also like to do your own monitoring, you can purchase a monitoring add-on, which will expose metrics for your own Prometheus instance. You can then set up a Grafana dashboard to view the metrics.</p> <p>The monitoring add-on provides:</p> <ul> <li>Control-plane metrics such as queue-depth, delayed VM launches, and failed jobs.</li> <li>Server metrics such as VMs running, load averages, and network I/O.</li> </ul> <p>To opt-in, follow the instructions in the dashboard.</p>"},{"location":"tasks/monitoring/#scrape-the-metrics","title":"Scrape the metrics","text":"<p>Metrics are currently made available through Prometheus federation. Prometheus can be run with Docker, as a Kubernetes deployment, or as a standalone binary.</p> <p>You can add a scrape target to your own Prometheus instance, or you can use the Grafana Agent to do that and ship off the metrics to Grafana Cloud.</p> <p>Here is a sample scrape config for Prometheus:</p> <pre><code>scrape_configs:\n- job_name: \"actuated\"\n\nbearer_token: \"xyz\"\nscheme: https\nmetrics_path: /api/v1/metrics/federate\n\n# Note: this value cannot be changed lower than 60s due to rate-limiting\nscrape_interval: 60s\nscrape_timeout: 5s\nstatic_configs:\n- targets: [\"tbc:443\"]\n</code></pre> <p>The <code>bearer_token</code> is a secret, and unique per customer. Only a bcrypt hash is stored in the control-plane, along with a mapping between GitHub organisations and the token.</p> <p>The <code>scrape_interval</code> must be <code>60s</code>, or higher to avoid rate-limiting.</p> <p>Contact the support team on Slack for the value for the <code>targets</code> field.</p>"},{"location":"tasks/monitoring/#control-plane-metrics","title":"Control-plane metrics","text":"<p>Check the pending build queue depth:</p> <pre><code>actuated_controller_db_queued_status&gt;0\n</code></pre> <p>Check for delayed VM launches (due to insufficient capacity):</p> <pre><code>sum by(owner) ( rate(actuated_controller_vm_launch_delayed_total\n[$__rate_interval]) )&gt;0\n</code></pre> <p>Failed jobs:</p> <pre><code>sum by (owner) (actuated_controller_job_failed_total{}) &gt; 0\n</code></pre> <p>Rate of jobs queued:</p> <pre><code>sum by(owner) ( rate(actuated_controller_job_queued_total[$__rate_interval]) ) &gt; 0   \n</code></pre> <p>Rate of VMs launched:</p> <pre><code>sum by(owner) (rate(actuated_controller_vm_launch_total [$__rate_interval]))&gt;0\n</code></pre>"},{"location":"tasks/monitoring/#server-metrics","title":"Server metrics","text":"<p>VMs running by host:</p> <pre><code>sum by(job) (actuated_vm_running_gauge) &gt; 0 \n</code></pre> <p>Free RAM by host:</p> <pre><code>actuated_system_memory_available_bytes{}\n</code></pre> <p>VM launch total:</p> <pre><code>sum by(job) ( actuated_vm_launch_total )\n</code></pre> <p>Load averages by host:</p> <pre><code>actuated_system_load_avg_1\n\nactuated_system_load_avg_5\n\nactuated_system_load_avg_15\n</code></pre> <p>Net I/O from egress adapter:</p> <pre><code>sum by( job) (  actuated_system_egress_rx )\n\nsum by( job) (  actuated_system_egress_tx )\n</code></pre>"},{"location":"tasks/registry-mirror/","title":"Example: Set up a registry mirror","text":"<p>Use-cases:</p> <ul> <li>Increase speed of pulls and builds by caching images on Actuated Servers</li> <li>Reduce failed builds due to rate-limiting</li> </ul> <p>If you use Docker in your self-hosted builds, there is a chance that you'll run into the rather conservative rate-limits.</p> <p>See also: Docker.com: Download rate limit</p> <p>The Docker Hub allows for 100 image pulls within a 6 hour period, but this can be extended to 200 by logging in, or to 5000 by paying for a Pro license.</p> <p>A registry mirror / pull-through cache running on an actuated agent is significantly faster than pulling from a remote server.</p> <p>We will create a mirror that:</p> <ul> <li>Has no authentication, to keep the changes to your build to a minimum</li> <li>Is read-only - for pulling images only</li> <li>Only has access to pull images from the Docker Hub</li> <li>Is not exposed to the Internet, but only to Actuated VMs</li> <li>When unavailable for any reason, the build continues without error</li> <li>Works on both Intel/AMD and ARM64 hosts</li> </ul> <p>This tutorial shows you how to set up what was previously known as \"Docker's Open Source Registry\" and is now a CNCF project called distribution.</p> <p>If you'd like to mirror another registry like gcr.io, ecr.io, quay.io, or your own registry, then you can use the same approach, but run each registry on a different port. The configuration may need to be set up manually, since the current action we have built is only designed for one mirror.</p> <p>Certified for:</p> <ul> <li> <code>x86_64</code></li> <li> <code>arm64</code></li> </ul>"},{"location":"tasks/registry-mirror/#create-a-docker-hub-access-token","title":"Create a Docker Hub Access token","text":"<p>Create a Docker Hub Access token with \"Public repos only\" scope, and save it as <code>~/hub.txt</code> on the Actuated Server.</p> <p></p> <p>Settings for an authorization token, with read-only permissions to public repositories</p>"},{"location":"tasks/registry-mirror/#set-up-the-registry-on-an-actuated-agent","title":"Set up the registry on an actuated agent","text":"<pre><code>(\ncurl -sLS https://get.arkade.dev | sudo sh\n\n  sudo arkade system install registry\n\n  sudo mkdir -p /etc/registry\n  sudo mkdir -p /var/lib/registry\n)\n</code></pre> <p>Create a config file to make the registry only available on the Linux bridge for Actuated VMs.</p> <p>Before doing so, you'll need to:</p> <ol> <li>Create a file named <code>hub.txt</code> in your home directory.</li> <li>Set the <code>USERNAME</code> variable to your Docker Hub username.</li> </ol> <pre><code>export USERNAME=\"\"\nexport TOKEN=$(cat ~/hub.txt)\n\ncat &gt;&gt; /tmp/registry.yml &lt;&lt;EOF\nversion: 0.1\nlog:\n  accesslog:\n    disabled: true\n  level: warn\n  formatter: text\n\nstorage:\n  filesystem:\n    rootdirectory: /var/lib/registry\n\nproxy:\n  remoteurl: https://registry-1.docker.io\n  username: $USERNAME\n\n  # A Docker Hub Personal Access token created with \"Public repos only\" scope\n  password: $TOKEN\n\nhttp:\n  addr: 192.168.128.1:5000\n  relativeurls: false\n  draintimeout: 60s\nEOF\n\nsudo mv /tmp/registry.yml /etc/registry/config.yml\n</code></pre> <p>Install and start the registry with a systemd unit file:</p> <pre><code>(\ncat &gt;&gt; /tmp/registry.service &lt;&lt;EOF\n[Unit]\nDescription=Registry\nAfter=network.target actuated.service\n\n[Service]\nType=simple\nRestart=always\nRestartSec=5s\nExecStart=/usr/local/bin/registry serve /etc/registry/config.yml\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo mv /tmp/registry.service /etc/systemd/system/registry.service\nsudo systemctl daemon-reload\nsudo systemctl enable registry --now\n)\n</code></pre> <p>Check the status with:</p> <pre><code>sudo journalctl -u registry -f\n</code></pre>"},{"location":"tasks/registry-mirror/#use-the-registry-within-a-workflow","title":"Use the registry within a workflow","text":"<p>Create a new registry in your organisation, along with a: <code>.github/workflows/build.yml</code> file and commit it to the repository.</p> <pre><code>name: CI\n\non:\npull_request:\nbranches:\n- '*'\npush:\nbranches:\n- master\n- main\n\njobs:\nbuild:\nruns-on: [actuated]\nsteps:\n\n- name: Setup mirror\nuses: self-actuated/hub-mirror@master\n\n- name: Checkout\nuses: actions/checkout@v2\n\n- name: Pull image using cache\nrun: |\ndocker pull alpine:latest\n</code></pre> <p>Note</p> <p>The <code>self-actuated/hub-mirror</code> action already runs the <code>docker/setup-buildx</code> action, so if you have that in your builds already, you can remove it, otherwise it will overwrite the settings for the mirror. Alternatively, move the <code>self-actuated/hub-mirror</code> action to after the <code>docker/setup-buildx</code> action.</p>"},{"location":"tasks/registry-mirror/#checking-if-it-worked","title":"Checking if it worked","text":"<p>You'll see the build run, and cached artifacts appearing in: <code>/var/lib/registry/</code>.</p> <pre><code>find /var/lib/registry/ -name \"alpine\"\n\n/var/lib/registry/docker/registry/v2/repositories/library/alpine\n</code></pre> <p>You can also use the registry's API to query which images are available:</p> <pre><code>curl -i http://192.168.128.1:5000/v2/_catalog\n\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nDocker-Distribution-Api-Version: registry/2.0\nDate: Wed, 16 Nov 2022 09:41:18 GMT\nContent-Length: 52\n\n{\"repositories\":[\"library/alpine\",\"moby/buildkit\"]}\n</code></pre> <p>You can check the status of the mirror at any time with:</p> <pre><code>sudo journalctl -u registry --since today\n</code></pre> <p>If you're not sure if the registry is working, or want to troubleshoot it, you can enable verbose logging, by editing the <code>log</code> section of the service file.</p> <pre><code>log:\n  accesslog:\n    disabled: false\n  level: debug\n  formatter: text\n</code></pre> <p>Then restart the service, and check the logs again. We do not recommend keeping this setting live as it will fill up the logs and disk quickly.</p>"},{"location":"tasks/registry-mirror/#a-note-on-kind","title":"A note on KinD","text":"<p>The self-actuated/hub-mirror action will configure both the Docker Daemon, and buildkit, however KinD uses its own instance of containerd and so must be configured separately.</p> <p>See notes on KinD with actuated for more information.</p>"},{"location":"tasks/registry-mirror/#further-reading","title":"Further reading","text":"<ul> <li>Docker: Configuration for the registry</li> <li>GitHub: View the project on GitHub</li> </ul>"}]}